{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/anaconda3/lib/python3.6/site-packages/lightgbm/__init__.py:46: UserWarning: Starting from version 2.2.1, the library file in distribution wheels for macOS is built by the Apple Clang (Xcode_9.4.1) compiler.\n",
      "This means that in case of installing LightGBM from PyPI via the ``pip install lightgbm`` command, you don't need to install the gcc compiler anymore.\n",
      "Instead of that, you need to install the OpenMP library, which is required for running LightGBM on the system with the Apple Clang compiler.\n",
      "You can install the OpenMP library by the following command: ``brew install libomp``.\n",
      "  \"You can install the OpenMP library by the following command: ``brew install libomp``.\", UserWarning)\n"
     ]
    }
   ],
   "source": [
    "# Importando bibliotecas\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn import linear_model\n",
    "import catboost as cat\n",
    "import pandas as pd\n",
    "import xgboost as xgb\n",
    "import lightgbm as lgb\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Lendo arquivos CSV\n",
    "df_train = pd.read_csv(\"/users/diegobernardo/downloads/base-santander/train-2.csv\")\n",
    "df_test  = pd.read_csv(\"/users/diegobernardo/downloads/base-santander/test-2.csv\")\n",
    "\n",
    "# Deletando a coluna ID\n",
    "df_train.drop(\"ID\", axis=1, inplace=True)\n",
    "\n",
    "# Armazenado o target em outra variável para excluir do dataframe de treinamento\n",
    "train_target = df_train['TARGET']\n",
    "df_train.drop(\"TARGET\", axis=1, inplace=True)\n",
    "\n",
    "# Armazenando o ID em outra variável para não ser usado no teste\n",
    "test_id = df_test['ID']\n",
    "df_test.drop('ID', axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Criando os folds\n",
    "def train_test_fold(n_folds, n_fold_test):\n",
    "    train = pd.DataFrame()\n",
    "    target = np.array([])\n",
    "    test = pd.DataFrame()\n",
    "\n",
    "    for i in range(n_folds):\n",
    "        inicio = df_train.shape[0]//n_folds*(i)\n",
    "        fim    = df_train.shape[0]//n_folds*(i+1)\n",
    "        if i == n_fold_test:\n",
    "            test = df_train.iloc[inicio:fim, :]\n",
    "        else:\n",
    "            train = pd.concat([train, df_train.iloc[inicio:fim, :]])\n",
    "            target = np.append(target, train_target[inicio:fim])\n",
    "\n",
    "    return train, target, test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [],
   "source": [
    "####################################################################################################################\n",
    "#######################################          XGBOOST          ##################################################\n",
    "####################################################################################################################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convertendo os dados para um estrutura que o XGBoost utiliza\n",
    "data_train_xgboost = xgb.DMatrix(data=df_train, label=train_target, weight=df_train['var15'])\n",
    "data_test_xgboost = xgb.DMatrix(data=df_test, weight=df_train['var15'])\n",
    "\n",
    "# Criando os parâmetros para o XGBoost\n",
    "objective = ['reg:logistic', 'binary:logistic']\n",
    "booster = ['gbtree', 'gblinear', 'dart']\n",
    "params_xgboost = []\n",
    "for obj in range(len(objective)):\n",
    "\n",
    "    for boo in range(len(booster)):\n",
    "        \n",
    "        for dep in range(2,16):\n",
    "    \n",
    "            for eta in range(1, 50, 5):\n",
    "                params_xgboost.append(\n",
    "                    { 'objective':objective[obj]\n",
    "                    , 'booster':booster[boo]\n",
    "                    , 'eta':eta/100\n",
    "                    , 'max_depth':dep\n",
    "                    , 'gamma':2\n",
    "                    , 'lambda':2\n",
    "                    , 'subsample':0.9\n",
    "                    , 'silent':True\n",
    "                    , 'colsample_bytree':0.50\n",
    "                    , 'colsample_bylevel':0.20\n",
    "                    , 'eval_metric':'auc'\n",
    "                    , 'seed': 1990}\n",
    "                )\n",
    "\n",
    "#params_xgboost = [\n",
    "#              { 'objective':'reg:logistic', 'booster':'gbtree', 'eta':0.01, 'max_depth':9, 'subsample':0.9, 'silent':True, 'colsample_bytree':0.50, 'colsample_bylevel':0.20, 'eval_metric':'auc', 'seed': 1990}\n",
    "#            , { 'objective':'reg:logistic', 'booster':'gbtree', 'eta':0.1,  'max_depth':9, 'subsample':0.9, 'silent':True, 'colsample_bytree':0.50, 'colsample_bylevel':0.20, 'eval_metric':'auc', 'seed': 1990}\n",
    "#            , { 'objective':'reg:logistic', 'booster':'gbtree', 'eta':0.4,  'max_depth':9, 'subsample':0.9, 'silent':True, 'colsample_bytree':0.50, 'colsample_bylevel':0.20, 'eval_metric':'auc', 'seed': 1990}\n",
    "#            , { 'objective':'reg:logistic', 'booster':'gbtree', 'eta':0.01, 'max_depth':6, 'subsample':0.9, 'silent':True, 'colsample_bytree':0.50, 'colsample_bylevel':0.20, 'eval_metric':'auc', 'seed': 1990}\n",
    "#            , { 'objective':'reg:logistic', 'booster':'gbtree', 'eta':0.1,  'max_depth':6, 'subsample':0.9, 'silent':True, 'colsample_bytree':0.50, 'colsample_bylevel':0.20, 'eval_metric':'auc', 'seed': 1990}\n",
    "#            , { 'objective':'reg:logistic', 'booster':'gbtree', 'eta':0.4,  'max_depth':6, 'subsample':0.9, 'silent':True, 'colsample_bytree':0.50, 'colsample_bylevel':0.20, 'eval_metric':'auc', 'seed': 1990}\n",
    "#        ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_round = 20\n",
    "n_folds = 10\n",
    "n_iteracoes = 30\n",
    "preds_xgboost_train = pd.DataFrame()\n",
    "preds_xgboost_test  = pd.DataFrame()\n",
    "\n",
    "# Loop de parâmetros\n",
    "for p in range(len(params_xgboost)):\n",
    "    \n",
    "    param = params_xgboost[p]\n",
    "    column = \"xgboost_p\" + str(p+1)\n",
    "    preds_param = np.array([])\n",
    "    \n",
    "    # Loop de Folds\n",
    "    for f in range(n_folds):\n",
    "        train, target, test = train_test_fold(n_folds, f)\n",
    "        train_xgboost = xgb.DMatrix(data=train, label=target, weight=train['var15'])\n",
    "        test_xgboost  = xgb.DMatrix(data=test, weight=test['var15'])\n",
    "        preds_fold = np.zeros(test_xgboost.num_row())\n",
    "\n",
    "        # Loop para retirar o ruído\n",
    "        for i in range(n_iteracoes): \n",
    "            param['seed'] = 1990+i\n",
    "            model_xgboost = xgb.train(param, train_xgboost, num_round)\n",
    "            preds_fold += model_xgboost.predict(test_xgboost)\n",
    "\n",
    "        preds_fold /= n_iteracoes\n",
    "        preds_param = np.append(preds_param, preds_fold)\n",
    "\n",
    "    print('XGBoost Param: ', p ,'/', len(params_xgboost))\n",
    "\n",
    "    meta_feature = pd.DataFrame(data=preds_param, columns=[column])\n",
    "    meta_feature.loc[meta_feature[column] > 1, column] = 1.0\n",
    "    meta_feature.loc[meta_feature[column] < 0, column] = 0.0\n",
    "    preds_xgboost_train = pd.concat([preds_xgboost_train, meta_feature], axis=1)\n",
    "    \n",
    "    \n",
    "    ############################################\n",
    "    ### CRIANDO AS FEATURES NA BASE DE TESTE ###\n",
    "    ############################################\n",
    "    \n",
    "    preds_param = np.zeros(data_test_xgboost.num_row())\n",
    "    \n",
    "    # Loop para retirar o ruído\n",
    "    for i in range(n_iteracoes):\n",
    "        param['seed'] = 1990+i\n",
    "        model_xgboost = xgb.train(param, data_train_xgboost, num_round)\n",
    "        preds_param += model_xgboost.predict(data_test_xgboost)\n",
    "        \n",
    "    preds_param /= n_iteracoes\n",
    "    meta_feature = pd.DataFrame(data=preds_param, columns=[column])\n",
    "    meta_feature.loc[meta_feature[column] > 1, column] = 1.0\n",
    "    meta_feature.loc[meta_feature[column] < 0, column] = 0.0\n",
    "    preds_xgboost_test = pd.concat([preds_xgboost_test, meta_feature], axis=1)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [],
   "source": [
    "####################################################################################################################\n",
    "#######################################          LIGHTGBM          #################################################\n",
    "####################################################################################################################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"\\nparams_lgbm = [\\n             { 'objective':'regression', 'boosting':'gbdt',         'learning_rate':0.01, 'max_depth':9, 'bagging_fraction':0.9, 'bagging_freq':10, 'colsample_bytree':0.50, 'metric':'auc', 'seed': 1990}\\n           , { 'objective':'regression', 'boosting':'gbdt',         'learning_rate':0.1,  'max_depth':9, 'bagging_fraction':0.9, 'bagging_freq':10, 'colsample_bytree':0.50, 'metric':'auc', 'seed': 1990} \\n           , { 'objective':'regression', 'boosting':'gbdt',         'learning_rate':0.4,  'max_depth':9, 'bagging_fraction':0.9, 'bagging_freq':10, 'colsample_bytree':0.50, 'metric':'auc', 'seed': 1990} \\n           , { 'objective':'regression', 'boosting':'gbdt',         'learning_rate':0.01, 'max_depth':6, 'bagging_fraction':0.9, 'bagging_freq':10, 'colsample_bytree':0.50, 'metric':'auc', 'seed': 1990}\\n           , { 'objective':'regression', 'boosting':'gbdt',         'learning_rate':0.1,  'max_depth':6, 'bagging_fraction':0.9, 'bagging_freq':10, 'colsample_bytree':0.50, 'metric':'auc', 'seed': 1990} \\n           , { 'objective':'regression', 'boosting':'gbdt',         'learning_rate':0.4,  'max_depth':6, 'bagging_fraction':0.9, 'bagging_freq':10, 'colsample_bytree':0.50, 'metric':'auc', 'seed': 1990} \\n           , { 'objective':'regression', 'boosting':'random_forest', 'learning_rate':0.01, 'max_depth':9, 'bagging_fraction':0.9, 'bagging_freq':10, 'colsample_bytree':0.50, 'metric':'auc', 'seed': 1990}\\n           , { 'objective':'regression', 'boosting':'random_forest', 'learning_rate':0.1,  'max_depth':9, 'bagging_fraction':0.9, 'bagging_freq':10, 'colsample_bytree':0.50, 'metric':'auc', 'seed': 1990} \\n           , { 'objective':'regression', 'boosting':'random_forest', 'learning_rate':0.4,  'max_depth':9, 'bagging_fraction':0.9, 'bagging_freq':10, 'colsample_bytree':0.50, 'metric':'auc', 'seed': 1990} \\n           , { 'objective':'regression', 'boosting':'random_forest', 'learning_rate':0.01, 'max_depth':6, 'bagging_fraction':0.9, 'bagging_freq':10, 'colsample_bytree':0.50, 'metric':'auc', 'seed': 1990}\\n           , { 'objective':'regression', 'boosting':'random_forest', 'learning_rate':0.1,  'max_depth':6, 'bagging_fraction':0.9, 'bagging_freq':10, 'colsample_bytree':0.50, 'metric':'auc', 'seed': 1990} \\n           , { 'objective':'regression', 'boosting':'random_forest', 'learning_rate':0.4,  'max_depth':6, 'bagging_fraction':0.9, 'bagging_freq':10, 'colsample_bytree':0.50, 'metric':'auc', 'seed': 1990} \\n\\n         ]\\n\""
      ]
     },
     "execution_count": 80,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Separando dados para treino e validação do treino\n",
    "x_train, x_valid, y_train, y_valid = train_test_split(df_train, train_target, test_size=0.2)\n",
    "\n",
    "# Convertendo os dados para um estrutura que o LightGBM utiliza\n",
    "data_train_lgbm = lgb.Dataset(x_train, label=y_train)\n",
    "data_valid_lgbm = lgb.Dataset(x_valid, label=y_valid)\n",
    "data_test_lgbm  = lgb.Dataset(df_test)\n",
    "\n",
    "# Criando os parâmetros para o LightGBM\n",
    "booster = ['gbdt', 'random_forest', 'dart', 'goss']\n",
    "params_lgbm = []\n",
    "\n",
    "for boo in range(len(booster)):\n",
    "\n",
    "    for dep in range(2,16):\n",
    "\n",
    "        for eta in range(1, 50, 5):\n",
    "            params_lgbm.append(\n",
    "                {  'objective':'regression'\n",
    "                 , 'boosting':booster[boo]\n",
    "                 , 'learning_rate':eta/100\n",
    "                 , 'max_depth':dep\n",
    "                 , 'lambda_l2':2\n",
    "                 , 'categorical_feature=name':'var15'\n",
    "                 , 'bagging_fraction':0.9\n",
    "                 , 'bagging_freq':10\n",
    "                 , 'colsample_bytree':0.50\n",
    "                 , 'metric':'auc'\n",
    "                 , 'seed': 1990}\n",
    "            )\n",
    "\n",
    "\"\"\"\n",
    "params_lgbm = [\n",
    "             { 'objective':'regression', 'boosting':'gbdt',         'learning_rate':0.01, 'max_depth':9, 'bagging_fraction':0.9, 'bagging_freq':10, 'colsample_bytree':0.50, 'metric':'auc', 'seed': 1990}\n",
    "           , { 'objective':'regression', 'boosting':'gbdt',         'learning_rate':0.1,  'max_depth':9, 'bagging_fraction':0.9, 'bagging_freq':10, 'colsample_bytree':0.50, 'metric':'auc', 'seed': 1990} \n",
    "           , { 'objective':'regression', 'boosting':'gbdt',         'learning_rate':0.4,  'max_depth':9, 'bagging_fraction':0.9, 'bagging_freq':10, 'colsample_bytree':0.50, 'metric':'auc', 'seed': 1990} \n",
    "           , { 'objective':'regression', 'boosting':'gbdt',         'learning_rate':0.01, 'max_depth':6, 'bagging_fraction':0.9, 'bagging_freq':10, 'colsample_bytree':0.50, 'metric':'auc', 'seed': 1990}\n",
    "           , { 'objective':'regression', 'boosting':'gbdt',         'learning_rate':0.1,  'max_depth':6, 'bagging_fraction':0.9, 'bagging_freq':10, 'colsample_bytree':0.50, 'metric':'auc', 'seed': 1990} \n",
    "           , { 'objective':'regression', 'boosting':'gbdt',         'learning_rate':0.4,  'max_depth':6, 'bagging_fraction':0.9, 'bagging_freq':10, 'colsample_bytree':0.50, 'metric':'auc', 'seed': 1990} \n",
    "           , { 'objective':'regression', 'boosting':'random_forest', 'learning_rate':0.01, 'max_depth':9, 'bagging_fraction':0.9, 'bagging_freq':10, 'colsample_bytree':0.50, 'metric':'auc', 'seed': 1990}\n",
    "           , { 'objective':'regression', 'boosting':'random_forest', 'learning_rate':0.1,  'max_depth':9, 'bagging_fraction':0.9, 'bagging_freq':10, 'colsample_bytree':0.50, 'metric':'auc', 'seed': 1990} \n",
    "           , { 'objective':'regression', 'boosting':'random_forest', 'learning_rate':0.4,  'max_depth':9, 'bagging_fraction':0.9, 'bagging_freq':10, 'colsample_bytree':0.50, 'metric':'auc', 'seed': 1990} \n",
    "           , { 'objective':'regression', 'boosting':'random_forest', 'learning_rate':0.01, 'max_depth':6, 'bagging_fraction':0.9, 'bagging_freq':10, 'colsample_bytree':0.50, 'metric':'auc', 'seed': 1990}\n",
    "           , { 'objective':'regression', 'boosting':'random_forest', 'learning_rate':0.1,  'max_depth':6, 'bagging_fraction':0.9, 'bagging_freq':10, 'colsample_bytree':0.50, 'metric':'auc', 'seed': 1990} \n",
    "           , { 'objective':'regression', 'boosting':'random_forest', 'learning_rate':0.4,  'max_depth':6, 'bagging_fraction':0.9, 'bagging_freq':10, 'colsample_bytree':0.50, 'metric':'auc', 'seed': 1990} \n",
    "\n",
    "         ]\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_round = 20\n",
    "n_folds = 10\n",
    "n_iteracoes = 30\n",
    "preds_lgbm_train = pd.DataFrame()\n",
    "preds_lgbm_test  = pd.DataFrame()\n",
    "\n",
    "# Loop de parâmetros \n",
    "for p in range(len(params_lgbm)):\n",
    "    \n",
    "    param = params_lgbm[p]\n",
    "    column = \"lightgbm_p\" + str(p+1)\n",
    "    preds_param = np.array([])\n",
    "\n",
    "    # Loop de Folds\n",
    "    for f in range(n_folds):\n",
    "        \n",
    "        # Separa os dados por folds\n",
    "        train, target, test = train_test_fold(n_folds, f)\n",
    "        \n",
    "        # Converte os dados para uma estrutura que o LightGBM utiliza\n",
    "        x_train, x_valid, y_train, y_valid = train_test_split(train, target, test_size=0.2)\n",
    "        train_lgbm = lgb.Dataset(x_train, label=y_train)\n",
    "        valid_lgbm = lgb.Dataset(x_valid, label=y_valid)\n",
    "        preds_fold = np.zeros(len(test))\n",
    "        \n",
    "        # Loop para retirar o ruído\n",
    "        for i in range(n_iteracoes): \n",
    "            param['seed'] = 1990+i\n",
    "            model_lgbm = lgb.train(param\n",
    "                       , train_lgbm\n",
    "                       , num_round\n",
    "                       , valid_sets=[train_lgbm, valid_lgbm]\n",
    "                       , valid_names=['train', 'valid']\n",
    "                       , early_stopping_rounds=50\n",
    "                       , verbose_eval=False)\n",
    "            preds_fold += model_lgbm.predict(test, num_iteration=model_lgbm.best_iteration)\n",
    "\n",
    "        preds_fold /= n_iteracoes\n",
    "        preds_param = np.append(preds_param, preds_fold)\n",
    "        \n",
    "    print('LightGBM Param: ', p ,'/', len(params_lgbm))\n",
    "        \n",
    "    meta_feature = pd.DataFrame(data=preds_param, columns=[column])\n",
    "    meta_feature.loc[meta_feature[column] > 1, column] = 1.0\n",
    "    meta_feature.loc[meta_feature[column] < 0, column] = 0.0\n",
    "    preds_lgbm_train = pd.concat([preds_lgbm_train, meta_feature], axis=1)\n",
    "\n",
    "\n",
    "    ############################################\n",
    "    ### CRIANDO AS FEATURES NA BASE DE TESTE ###\n",
    "    ############################################\n",
    "    \n",
    "    preds_param = np.zeros(df_test.shape[0])\n",
    "    \n",
    "    # Loop para retirar o ruído\n",
    "    for i in range(n_iteracoes):\n",
    "        param['seed'] = 1990+i\n",
    "        model_lgbm = lgb.train(param\n",
    "                                , data_train_lgbm\n",
    "                                , num_round\n",
    "                                , valid_sets=[data_train_lgbm, data_valid_lgbm]\n",
    "                                , valid_names=['train', 'valid']\n",
    "                                , early_stopping_rounds=50\n",
    "                                , verbose_eval=False)\n",
    "        preds_param += model_lgbm.predict(df_test, num_iteration=model_lgbm.best_iteration)\n",
    "        \n",
    "    preds_param /= n_iteracoes\n",
    "    meta_feature = pd.DataFrame(data=preds_param, columns=[column])\n",
    "    meta_feature.loc[meta_feature[column] > 1, column] = 1.0\n",
    "    meta_feature.loc[meta_feature[column] < 0, column] = 0.0\n",
    "    preds_lgbm_test = pd.concat([preds_lgbm_test, meta_feature], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "####################################################################################################################\n",
    "#######################################          CATBOOST         ##################################################\n",
    "####################################################################################################################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CatBoost Param:  0 / 2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Iteration with suspicious time -1.32e+04 sec ignored in overall statistics.\n",
      "\n",
      "Iteration with suspicious time -35.6 sec ignored in overall statistics.\n",
      "\n",
      "Iteration with suspicious time -43.6 sec ignored in overall statistics.\n",
      "\n",
      "Iteration with suspicious time -0.93 sec ignored in overall statistics.\n"
     ]
    }
   ],
   "source": [
    "\n",
    "params_cat = [\n",
    "             { 'objective':'RMSE', 'custom_metric':'AUC', 'eval_metric':'AUC', 'learning_rate':0.01, 'random_seed':1990, 'max_depth':6, 'colsample_bylevel':0.5}\n",
    "            #,{ 'objective':'RMSE', 'custom_metric':'AUC', 'eval_metric':'AUC', 'learning_rate':0.1, 'random_seed':1990, 'max_depth':6, 'colsample_bylevel':0.5}\n",
    "            #,{ 'objective':'RMSE', 'custom_metric':'AUC', 'eval_metric':'AUC', 'learning_rate':0.4, 'random_seed':1990, 'max_depth':6, 'colsample_bylevel':0.5}\n",
    "            #,{ 'objective':'RMSE', 'custom_metric':'AUC', 'eval_metric':'AUC', 'learning_rate':0.01, 'random_seed':1990, 'max_depth':9, 'colsample_bylevel':0.5}\n",
    "            #,{ 'objective':'RMSE', 'custom_metric':'AUC', 'eval_metric':'AUC', 'learning_rate':0.1, 'random_seed':1990, 'max_depth':9, 'colsample_bylevel':0.5}\n",
    "            #,{ 'objective':'RMSE', 'custom_metric':'AUC', 'eval_metric':'AUC', 'learning_rate':0.4, 'random_seed':1990, 'max_depth':9, 'colsample_bylevel':0.5}\n",
    "            #,{ 'objective':'RMSE', 'custom_metric':'AUC', 'eval_metric':'AUC', 'learning_rate':0.01, 'random_seed':1990, 'max_depth':12, 'colsample_bylevel':0.5}\n",
    "            #,{ 'objective':'RMSE', 'custom_metric':'AUC', 'eval_metric':'AUC', 'learning_rate':0.1, 'random_seed':1990, 'max_depth':12, 'colsample_bylevel':0.5}\n",
    "            #,{ 'objective':'RMSE', 'custom_metric':'AUC', 'eval_metric':'AUC', 'learning_rate':0.4, 'random_seed':1990, 'max_depth':12, 'colsample_bylevel':0.5}\n",
    "            #,{ 'objective':'RMSE', 'custom_metric':'AUC', 'eval_metric':'AUC', 'learning_rate':0.01, 'random_seed':1990, 'max_depth':15, 'colsample_bylevel':0.5}\n",
    "            #,{ 'objective':'RMSE', 'custom_metric':'AUC', 'eval_metric':'AUC', 'learning_rate':0.1, 'random_seed':1990, 'max_depth':15, 'colsample_bylevel':0.5}\n",
    "            ,{ 'objective':'RMSE', 'custom_metric':'AUC', 'eval_metric':'AUC', 'learning_rate':0.4, 'random_seed':1990, 'max_depth':15, 'colsample_bylevel':0.5}\n",
    "        ]\n",
    "\n",
    "num_round = 20\n",
    "n_folds = 10\n",
    "n_iteracoes = 30\n",
    "preds_catboost_train = pd.DataFrame()\n",
    "preds_catboost_test  = pd.DataFrame()\n",
    "data_train_catboost  = cat.Pool(df_train, label=train_target)\n",
    "data_test_catboost   = cat.Pool(df_test)\n",
    "\n",
    "# Loop de parâmetros \n",
    "for p in range(len(params_cat)):\n",
    "    \n",
    "    param = params_cat[p]\n",
    "    column = \"catboost_p\" + str(p+1)\n",
    "    preds_param = np.array([])\n",
    "\n",
    "    # Loop de Folds\n",
    "    for f in range(n_folds):\n",
    "        \n",
    "        # Separa os dados por folds\n",
    "        train, target, test = train_test_fold(n_folds, f)\n",
    "        \n",
    "        # Converte os dados para uma estrutura que o CATBOOST utiliza\n",
    "        train_catboost = cat.Pool(train, label=target)\n",
    "        test_catboost  = cat.Pool(test)\n",
    "        preds_fold = np.zeros(test_catboost.num_row())\n",
    "        \n",
    "        # Loop para retirar o ruído\n",
    "        for i in range(n_iteracoes): \n",
    "            param['random_seed'] = 1990+i\n",
    "            model_catboost = cat.train(params=param, pool=train_catboost, num_boost_round=num_round, logging_level='Silent')\n",
    "            preds_fold += model_catboost.predict(test_catboost)\n",
    "            \n",
    "        preds_fold /= n_iteracoes\n",
    "        preds_param = np.append(preds_param, preds_fold)\n",
    "        \n",
    "    print('CatBoost Param: ', p ,'/', len(params_cat))\n",
    "        \n",
    "    meta_feature = pd.DataFrame(data=preds_param, columns=[column])\n",
    "    meta_feature.loc[meta_feature[column] > 1, column] = 1.0\n",
    "    meta_feature.loc[meta_feature[column] < 0, column] = 0.0\n",
    "    preds_catboost_train = pd.concat([preds_catboost_train, meta_feature], axis=1)\n",
    "    \n",
    "    \n",
    "    ############################################\n",
    "    ### CRIANDO AS FEATURES NA BASE DE TESTE ###\n",
    "    ############################################\n",
    "    \n",
    "    preds_param = np.zeros(data_test_catboost.num_row())\n",
    "    \n",
    "    # Loop para retirar o ruído\n",
    "    for i in range(n_iteracoes):\n",
    "        param['random_seed'] = 1990+i\n",
    "        model_catboost = cat.train(params=param, pool=data_train_catboost, num_boost_round=num_round, logging_level='Silent')\n",
    "        preds_param += model_catboost.predict(data_test_catboost)\n",
    "        \n",
    "    preds_param /= n_iteracoes\n",
    "    meta_feature = pd.DataFrame(data=preds_param, columns=[column])\n",
    "    meta_feature.loc[meta_feature[column] > 1, column] = 1.0\n",
    "    meta_feature.loc[meta_feature[column] < 0, column] = 0.0\n",
    "    preds_catboost_test = pd.concat([preds_catboost_test, meta_feature], axis=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "####################################################################################################################\n",
    "########################     JUNTANDO AS META FEATURES DO XGBOOST - LIGHTGBM E CATBOOST     ########################\n",
    "####################################################################################################################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>xgboost_p1</th>\n",
       "      <th>xgboost_p2</th>\n",
       "      <th>xgboost_p3</th>\n",
       "      <th>xgboost_p4</th>\n",
       "      <th>xgboost_p5</th>\n",
       "      <th>xgboost_p6</th>\n",
       "      <th>lightgbm_p1</th>\n",
       "      <th>lightgbm_p2</th>\n",
       "      <th>lightgbm_p3</th>\n",
       "      <th>lightgbm_p4</th>\n",
       "      <th>...</th>\n",
       "      <th>lightgbm_p9</th>\n",
       "      <th>lightgbm_p10</th>\n",
       "      <th>lightgbm_p11</th>\n",
       "      <th>lightgbm_p12</th>\n",
       "      <th>catboost_p1</th>\n",
       "      <th>catboost_p2</th>\n",
       "      <th>catboost_p3</th>\n",
       "      <th>catboost_p4</th>\n",
       "      <th>catboost_p5</th>\n",
       "      <th>catboost_p6</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.415721</td>\n",
       "      <td>0.097202</td>\n",
       "      <td>0.042828</td>\n",
       "      <td>0.415151</td>\n",
       "      <td>0.095824</td>\n",
       "      <td>0.051976</td>\n",
       "      <td>0.038326</td>\n",
       "      <td>0.036761</td>\n",
       "      <td>0.044406</td>\n",
       "      <td>0.038865</td>\n",
       "      <td>...</td>\n",
       "      <td>0.033345</td>\n",
       "      <td>0.037256</td>\n",
       "      <td>0.037256</td>\n",
       "      <td>0.037256</td>\n",
       "      <td>0.009733</td>\n",
       "      <td>0.047941</td>\n",
       "      <td>0.056483</td>\n",
       "      <td>0.008408</td>\n",
       "      <td>0.044714</td>\n",
       "      <td>0.050532</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.416529</td>\n",
       "      <td>0.100956</td>\n",
       "      <td>0.052814</td>\n",
       "      <td>0.415314</td>\n",
       "      <td>0.097803</td>\n",
       "      <td>0.055960</td>\n",
       "      <td>0.039659</td>\n",
       "      <td>0.042465</td>\n",
       "      <td>0.063659</td>\n",
       "      <td>0.040003</td>\n",
       "      <td>...</td>\n",
       "      <td>0.043101</td>\n",
       "      <td>0.045748</td>\n",
       "      <td>0.045748</td>\n",
       "      <td>0.045748</td>\n",
       "      <td>0.009770</td>\n",
       "      <td>0.048732</td>\n",
       "      <td>0.057247</td>\n",
       "      <td>0.008675</td>\n",
       "      <td>0.047394</td>\n",
       "      <td>0.054229</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.411698</td>\n",
       "      <td>0.075681</td>\n",
       "      <td>0.002523</td>\n",
       "      <td>0.412123</td>\n",
       "      <td>0.078379</td>\n",
       "      <td>0.005697</td>\n",
       "      <td>0.034558</td>\n",
       "      <td>0.009907</td>\n",
       "      <td>0.000417</td>\n",
       "      <td>0.034524</td>\n",
       "      <td>...</td>\n",
       "      <td>0.009579</td>\n",
       "      <td>0.008015</td>\n",
       "      <td>0.008015</td>\n",
       "      <td>0.008015</td>\n",
       "      <td>0.001534</td>\n",
       "      <td>0.005833</td>\n",
       "      <td>0.004568</td>\n",
       "      <td>0.001419</td>\n",
       "      <td>0.004775</td>\n",
       "      <td>0.002751</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.418683</td>\n",
       "      <td>0.101366</td>\n",
       "      <td>0.014062</td>\n",
       "      <td>0.420119</td>\n",
       "      <td>0.107787</td>\n",
       "      <td>0.017989</td>\n",
       "      <td>0.038399</td>\n",
       "      <td>0.022037</td>\n",
       "      <td>0.006973</td>\n",
       "      <td>0.038076</td>\n",
       "      <td>...</td>\n",
       "      <td>0.035022</td>\n",
       "      <td>0.030968</td>\n",
       "      <td>0.030968</td>\n",
       "      <td>0.030968</td>\n",
       "      <td>0.002671</td>\n",
       "      <td>0.010831</td>\n",
       "      <td>0.010048</td>\n",
       "      <td>0.002452</td>\n",
       "      <td>0.009966</td>\n",
       "      <td>0.009669</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.411532</td>\n",
       "      <td>0.075520</td>\n",
       "      <td>0.003606</td>\n",
       "      <td>0.412088</td>\n",
       "      <td>0.078156</td>\n",
       "      <td>0.006529</td>\n",
       "      <td>0.034534</td>\n",
       "      <td>0.010064</td>\n",
       "      <td>0.001004</td>\n",
       "      <td>0.034572</td>\n",
       "      <td>...</td>\n",
       "      <td>0.009462</td>\n",
       "      <td>0.008250</td>\n",
       "      <td>0.008250</td>\n",
       "      <td>0.008250</td>\n",
       "      <td>0.001609</td>\n",
       "      <td>0.005400</td>\n",
       "      <td>0.003600</td>\n",
       "      <td>0.001456</td>\n",
       "      <td>0.004505</td>\n",
       "      <td>0.002639</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>0.436388</td>\n",
       "      <td>0.207404</td>\n",
       "      <td>0.223897</td>\n",
       "      <td>0.434440</td>\n",
       "      <td>0.197179</td>\n",
       "      <td>0.219029</td>\n",
       "      <td>0.059274</td>\n",
       "      <td>0.171075</td>\n",
       "      <td>0.225850</td>\n",
       "      <td>0.057470</td>\n",
       "      <td>...</td>\n",
       "      <td>0.166743</td>\n",
       "      <td>0.162629</td>\n",
       "      <td>0.162629</td>\n",
       "      <td>0.162629</td>\n",
       "      <td>0.036364</td>\n",
       "      <td>0.191683</td>\n",
       "      <td>0.229837</td>\n",
       "      <td>0.039539</td>\n",
       "      <td>0.197686</td>\n",
       "      <td>0.235108</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>0.417094</td>\n",
       "      <td>0.102855</td>\n",
       "      <td>0.038031</td>\n",
       "      <td>0.417951</td>\n",
       "      <td>0.108334</td>\n",
       "      <td>0.022224</td>\n",
       "      <td>0.043018</td>\n",
       "      <td>0.063572</td>\n",
       "      <td>0.056381</td>\n",
       "      <td>0.043660</td>\n",
       "      <td>...</td>\n",
       "      <td>0.060888</td>\n",
       "      <td>0.065457</td>\n",
       "      <td>0.065457</td>\n",
       "      <td>0.065457</td>\n",
       "      <td>0.008193</td>\n",
       "      <td>0.047765</td>\n",
       "      <td>0.065961</td>\n",
       "      <td>0.011391</td>\n",
       "      <td>0.060371</td>\n",
       "      <td>0.066223</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>0.434541</td>\n",
       "      <td>0.188525</td>\n",
       "      <td>0.165028</td>\n",
       "      <td>0.433910</td>\n",
       "      <td>0.188646</td>\n",
       "      <td>0.164309</td>\n",
       "      <td>0.053870</td>\n",
       "      <td>0.128298</td>\n",
       "      <td>0.159471</td>\n",
       "      <td>0.055220</td>\n",
       "      <td>...</td>\n",
       "      <td>0.131200</td>\n",
       "      <td>0.147904</td>\n",
       "      <td>0.147904</td>\n",
       "      <td>0.147904</td>\n",
       "      <td>0.035928</td>\n",
       "      <td>0.138741</td>\n",
       "      <td>0.158281</td>\n",
       "      <td>0.037659</td>\n",
       "      <td>0.140164</td>\n",
       "      <td>0.157870</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>0.412955</td>\n",
       "      <td>0.084585</td>\n",
       "      <td>0.023091</td>\n",
       "      <td>0.413026</td>\n",
       "      <td>0.084503</td>\n",
       "      <td>0.027572</td>\n",
       "      <td>0.035828</td>\n",
       "      <td>0.022094</td>\n",
       "      <td>0.021774</td>\n",
       "      <td>0.036197</td>\n",
       "      <td>...</td>\n",
       "      <td>0.017801</td>\n",
       "      <td>0.019401</td>\n",
       "      <td>0.019401</td>\n",
       "      <td>0.019401</td>\n",
       "      <td>0.004148</td>\n",
       "      <td>0.025217</td>\n",
       "      <td>0.029418</td>\n",
       "      <td>0.004187</td>\n",
       "      <td>0.022857</td>\n",
       "      <td>0.029823</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>0.413165</td>\n",
       "      <td>0.083132</td>\n",
       "      <td>0.012546</td>\n",
       "      <td>0.413612</td>\n",
       "      <td>0.086278</td>\n",
       "      <td>0.021072</td>\n",
       "      <td>0.035916</td>\n",
       "      <td>0.021287</td>\n",
       "      <td>0.022722</td>\n",
       "      <td>0.036292</td>\n",
       "      <td>...</td>\n",
       "      <td>0.018326</td>\n",
       "      <td>0.020287</td>\n",
       "      <td>0.020287</td>\n",
       "      <td>0.020287</td>\n",
       "      <td>0.004147</td>\n",
       "      <td>0.024443</td>\n",
       "      <td>0.027147</td>\n",
       "      <td>0.004252</td>\n",
       "      <td>0.022383</td>\n",
       "      <td>0.022125</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>0.413652</td>\n",
       "      <td>0.085491</td>\n",
       "      <td>0.026448</td>\n",
       "      <td>0.414012</td>\n",
       "      <td>0.086991</td>\n",
       "      <td>0.026322</td>\n",
       "      <td>0.037136</td>\n",
       "      <td>0.025133</td>\n",
       "      <td>0.026086</td>\n",
       "      <td>0.037477</td>\n",
       "      <td>...</td>\n",
       "      <td>0.026272</td>\n",
       "      <td>0.028518</td>\n",
       "      <td>0.028518</td>\n",
       "      <td>0.028518</td>\n",
       "      <td>0.001475</td>\n",
       "      <td>0.018881</td>\n",
       "      <td>0.021307</td>\n",
       "      <td>0.001319</td>\n",
       "      <td>0.017480</td>\n",
       "      <td>0.018862</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>0.412398</td>\n",
       "      <td>0.079559</td>\n",
       "      <td>0.005860</td>\n",
       "      <td>0.412624</td>\n",
       "      <td>0.081404</td>\n",
       "      <td>0.009184</td>\n",
       "      <td>0.034700</td>\n",
       "      <td>0.012075</td>\n",
       "      <td>0.004345</td>\n",
       "      <td>0.034746</td>\n",
       "      <td>...</td>\n",
       "      <td>0.010277</td>\n",
       "      <td>0.009081</td>\n",
       "      <td>0.009081</td>\n",
       "      <td>0.009081</td>\n",
       "      <td>0.001777</td>\n",
       "      <td>0.006913</td>\n",
       "      <td>0.006437</td>\n",
       "      <td>0.001745</td>\n",
       "      <td>0.005949</td>\n",
       "      <td>0.005329</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>0.413001</td>\n",
       "      <td>0.083037</td>\n",
       "      <td>0.009617</td>\n",
       "      <td>0.413477</td>\n",
       "      <td>0.084387</td>\n",
       "      <td>0.018354</td>\n",
       "      <td>0.035928</td>\n",
       "      <td>0.022697</td>\n",
       "      <td>0.022236</td>\n",
       "      <td>0.036316</td>\n",
       "      <td>...</td>\n",
       "      <td>0.018324</td>\n",
       "      <td>0.020237</td>\n",
       "      <td>0.020237</td>\n",
       "      <td>0.020237</td>\n",
       "      <td>0.004193</td>\n",
       "      <td>0.020361</td>\n",
       "      <td>0.019274</td>\n",
       "      <td>0.004568</td>\n",
       "      <td>0.018458</td>\n",
       "      <td>0.019071</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>0.411574</td>\n",
       "      <td>0.076538</td>\n",
       "      <td>0.010394</td>\n",
       "      <td>0.412024</td>\n",
       "      <td>0.078091</td>\n",
       "      <td>0.010398</td>\n",
       "      <td>0.034538</td>\n",
       "      <td>0.012004</td>\n",
       "      <td>0.007478</td>\n",
       "      <td>0.034834</td>\n",
       "      <td>...</td>\n",
       "      <td>0.009462</td>\n",
       "      <td>0.010097</td>\n",
       "      <td>0.010097</td>\n",
       "      <td>0.010097</td>\n",
       "      <td>0.001620</td>\n",
       "      <td>0.007365</td>\n",
       "      <td>0.008802</td>\n",
       "      <td>0.001521</td>\n",
       "      <td>0.006905</td>\n",
       "      <td>0.008265</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>0.411586</td>\n",
       "      <td>0.075089</td>\n",
       "      <td>0.034677</td>\n",
       "      <td>0.412344</td>\n",
       "      <td>0.078928</td>\n",
       "      <td>0.004842</td>\n",
       "      <td>0.035008</td>\n",
       "      <td>0.017095</td>\n",
       "      <td>0.018469</td>\n",
       "      <td>0.035232</td>\n",
       "      <td>...</td>\n",
       "      <td>0.012412</td>\n",
       "      <td>0.012376</td>\n",
       "      <td>0.012376</td>\n",
       "      <td>0.012376</td>\n",
       "      <td>0.004563</td>\n",
       "      <td>0.020259</td>\n",
       "      <td>0.023014</td>\n",
       "      <td>0.004628</td>\n",
       "      <td>0.018797</td>\n",
       "      <td>0.022652</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>0.414358</td>\n",
       "      <td>0.087206</td>\n",
       "      <td>0.055722</td>\n",
       "      <td>0.414715</td>\n",
       "      <td>0.090410</td>\n",
       "      <td>0.022342</td>\n",
       "      <td>0.036133</td>\n",
       "      <td>0.024514</td>\n",
       "      <td>0.024004</td>\n",
       "      <td>0.035950</td>\n",
       "      <td>...</td>\n",
       "      <td>0.019450</td>\n",
       "      <td>0.017882</td>\n",
       "      <td>0.017882</td>\n",
       "      <td>0.017882</td>\n",
       "      <td>0.003745</td>\n",
       "      <td>0.013890</td>\n",
       "      <td>0.022812</td>\n",
       "      <td>0.004255</td>\n",
       "      <td>0.016833</td>\n",
       "      <td>0.026008</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>0.425593</td>\n",
       "      <td>0.152285</td>\n",
       "      <td>0.140102</td>\n",
       "      <td>0.423683</td>\n",
       "      <td>0.141453</td>\n",
       "      <td>0.138531</td>\n",
       "      <td>0.047131</td>\n",
       "      <td>0.098496</td>\n",
       "      <td>0.126048</td>\n",
       "      <td>0.046789</td>\n",
       "      <td>...</td>\n",
       "      <td>0.088449</td>\n",
       "      <td>0.089007</td>\n",
       "      <td>0.089007</td>\n",
       "      <td>0.089007</td>\n",
       "      <td>0.023202</td>\n",
       "      <td>0.110588</td>\n",
       "      <td>0.134165</td>\n",
       "      <td>0.019099</td>\n",
       "      <td>0.104189</td>\n",
       "      <td>0.132489</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>0.411404</td>\n",
       "      <td>0.075149</td>\n",
       "      <td>0.004505</td>\n",
       "      <td>0.411951</td>\n",
       "      <td>0.077534</td>\n",
       "      <td>0.006633</td>\n",
       "      <td>0.034534</td>\n",
       "      <td>0.010024</td>\n",
       "      <td>0.001414</td>\n",
       "      <td>0.034572</td>\n",
       "      <td>...</td>\n",
       "      <td>0.009462</td>\n",
       "      <td>0.008250</td>\n",
       "      <td>0.008250</td>\n",
       "      <td>0.008250</td>\n",
       "      <td>0.001432</td>\n",
       "      <td>0.005308</td>\n",
       "      <td>0.003673</td>\n",
       "      <td>0.001302</td>\n",
       "      <td>0.004738</td>\n",
       "      <td>0.003311</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>0.411590</td>\n",
       "      <td>0.076757</td>\n",
       "      <td>0.012043</td>\n",
       "      <td>0.412022</td>\n",
       "      <td>0.078281</td>\n",
       "      <td>0.012488</td>\n",
       "      <td>0.034538</td>\n",
       "      <td>0.012548</td>\n",
       "      <td>0.011136</td>\n",
       "      <td>0.034834</td>\n",
       "      <td>...</td>\n",
       "      <td>0.009462</td>\n",
       "      <td>0.010097</td>\n",
       "      <td>0.010097</td>\n",
       "      <td>0.010097</td>\n",
       "      <td>0.001737</td>\n",
       "      <td>0.008664</td>\n",
       "      <td>0.011906</td>\n",
       "      <td>0.001757</td>\n",
       "      <td>0.008469</td>\n",
       "      <td>0.012168</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>0.412702</td>\n",
       "      <td>0.082277</td>\n",
       "      <td>0.009676</td>\n",
       "      <td>0.413057</td>\n",
       "      <td>0.084306</td>\n",
       "      <td>0.017678</td>\n",
       "      <td>0.035805</td>\n",
       "      <td>0.019002</td>\n",
       "      <td>0.018259</td>\n",
       "      <td>0.036168</td>\n",
       "      <td>...</td>\n",
       "      <td>0.017733</td>\n",
       "      <td>0.019200</td>\n",
       "      <td>0.019200</td>\n",
       "      <td>0.019200</td>\n",
       "      <td>0.003252</td>\n",
       "      <td>0.016323</td>\n",
       "      <td>0.021822</td>\n",
       "      <td>0.002777</td>\n",
       "      <td>0.016189</td>\n",
       "      <td>0.021345</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>0.413684</td>\n",
       "      <td>0.085181</td>\n",
       "      <td>0.021388</td>\n",
       "      <td>0.414113</td>\n",
       "      <td>0.087387</td>\n",
       "      <td>0.025052</td>\n",
       "      <td>0.036990</td>\n",
       "      <td>0.022865</td>\n",
       "      <td>0.019262</td>\n",
       "      <td>0.037292</td>\n",
       "      <td>...</td>\n",
       "      <td>0.025554</td>\n",
       "      <td>0.027936</td>\n",
       "      <td>0.027936</td>\n",
       "      <td>0.027936</td>\n",
       "      <td>0.001676</td>\n",
       "      <td>0.017952</td>\n",
       "      <td>0.017277</td>\n",
       "      <td>0.001662</td>\n",
       "      <td>0.017084</td>\n",
       "      <td>0.016626</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>0.434643</td>\n",
       "      <td>0.193682</td>\n",
       "      <td>0.180755</td>\n",
       "      <td>0.433105</td>\n",
       "      <td>0.185280</td>\n",
       "      <td>0.172130</td>\n",
       "      <td>0.055687</td>\n",
       "      <td>0.148467</td>\n",
       "      <td>0.191709</td>\n",
       "      <td>0.054306</td>\n",
       "      <td>...</td>\n",
       "      <td>0.143836</td>\n",
       "      <td>0.140858</td>\n",
       "      <td>0.140858</td>\n",
       "      <td>0.140858</td>\n",
       "      <td>0.029655</td>\n",
       "      <td>0.145261</td>\n",
       "      <td>0.172555</td>\n",
       "      <td>0.030307</td>\n",
       "      <td>0.147457</td>\n",
       "      <td>0.176705</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>0.423064</td>\n",
       "      <td>0.123198</td>\n",
       "      <td>0.031013</td>\n",
       "      <td>0.424802</td>\n",
       "      <td>0.130981</td>\n",
       "      <td>0.036904</td>\n",
       "      <td>0.041722</td>\n",
       "      <td>0.048125</td>\n",
       "      <td>0.033073</td>\n",
       "      <td>0.041093</td>\n",
       "      <td>...</td>\n",
       "      <td>0.054546</td>\n",
       "      <td>0.050185</td>\n",
       "      <td>0.050185</td>\n",
       "      <td>0.050185</td>\n",
       "      <td>0.006553</td>\n",
       "      <td>0.028691</td>\n",
       "      <td>0.031953</td>\n",
       "      <td>0.006743</td>\n",
       "      <td>0.029492</td>\n",
       "      <td>0.035461</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>0.411123</td>\n",
       "      <td>0.072142</td>\n",
       "      <td>0.001795</td>\n",
       "      <td>0.412533</td>\n",
       "      <td>0.078629</td>\n",
       "      <td>0.002506</td>\n",
       "      <td>0.035297</td>\n",
       "      <td>0.018038</td>\n",
       "      <td>0.009790</td>\n",
       "      <td>0.035162</td>\n",
       "      <td>...</td>\n",
       "      <td>0.014269</td>\n",
       "      <td>0.012509</td>\n",
       "      <td>0.012509</td>\n",
       "      <td>0.012509</td>\n",
       "      <td>0.003580</td>\n",
       "      <td>0.014593</td>\n",
       "      <td>0.015703</td>\n",
       "      <td>0.003407</td>\n",
       "      <td>0.013434</td>\n",
       "      <td>0.012804</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>0.420855</td>\n",
       "      <td>0.119755</td>\n",
       "      <td>0.034098</td>\n",
       "      <td>0.423301</td>\n",
       "      <td>0.128813</td>\n",
       "      <td>0.070650</td>\n",
       "      <td>0.043696</td>\n",
       "      <td>0.069506</td>\n",
       "      <td>0.087991</td>\n",
       "      <td>0.046068</td>\n",
       "      <td>...</td>\n",
       "      <td>0.067733</td>\n",
       "      <td>0.085264</td>\n",
       "      <td>0.085264</td>\n",
       "      <td>0.085264</td>\n",
       "      <td>0.019712</td>\n",
       "      <td>0.095134</td>\n",
       "      <td>0.107975</td>\n",
       "      <td>0.016086</td>\n",
       "      <td>0.080713</td>\n",
       "      <td>0.095997</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>0.412449</td>\n",
       "      <td>0.078390</td>\n",
       "      <td>0.002095</td>\n",
       "      <td>0.413154</td>\n",
       "      <td>0.082205</td>\n",
       "      <td>0.009346</td>\n",
       "      <td>0.035651</td>\n",
       "      <td>0.017027</td>\n",
       "      <td>0.007619</td>\n",
       "      <td>0.035306</td>\n",
       "      <td>...</td>\n",
       "      <td>0.016286</td>\n",
       "      <td>0.013085</td>\n",
       "      <td>0.013085</td>\n",
       "      <td>0.013085</td>\n",
       "      <td>0.003020</td>\n",
       "      <td>0.011742</td>\n",
       "      <td>0.008326</td>\n",
       "      <td>0.002444</td>\n",
       "      <td>0.009733</td>\n",
       "      <td>0.002689</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>0.411448</td>\n",
       "      <td>0.075552</td>\n",
       "      <td>0.004883</td>\n",
       "      <td>0.411983</td>\n",
       "      <td>0.077790</td>\n",
       "      <td>0.007308</td>\n",
       "      <td>0.034534</td>\n",
       "      <td>0.010024</td>\n",
       "      <td>0.001606</td>\n",
       "      <td>0.034572</td>\n",
       "      <td>...</td>\n",
       "      <td>0.009462</td>\n",
       "      <td>0.008250</td>\n",
       "      <td>0.008250</td>\n",
       "      <td>0.008250</td>\n",
       "      <td>0.001472</td>\n",
       "      <td>0.005631</td>\n",
       "      <td>0.004521</td>\n",
       "      <td>0.001389</td>\n",
       "      <td>0.005615</td>\n",
       "      <td>0.004949</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>0.411415</td>\n",
       "      <td>0.075387</td>\n",
       "      <td>0.008137</td>\n",
       "      <td>0.411955</td>\n",
       "      <td>0.077557</td>\n",
       "      <td>0.007026</td>\n",
       "      <td>0.034534</td>\n",
       "      <td>0.010024</td>\n",
       "      <td>0.001483</td>\n",
       "      <td>0.034572</td>\n",
       "      <td>...</td>\n",
       "      <td>0.009462</td>\n",
       "      <td>0.008250</td>\n",
       "      <td>0.008250</td>\n",
       "      <td>0.008250</td>\n",
       "      <td>0.001438</td>\n",
       "      <td>0.005665</td>\n",
       "      <td>0.004682</td>\n",
       "      <td>0.001303</td>\n",
       "      <td>0.005046</td>\n",
       "      <td>0.004602</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>0.436208</td>\n",
       "      <td>0.201367</td>\n",
       "      <td>0.174822</td>\n",
       "      <td>0.434546</td>\n",
       "      <td>0.193467</td>\n",
       "      <td>0.175310</td>\n",
       "      <td>0.057762</td>\n",
       "      <td>0.155047</td>\n",
       "      <td>0.178947</td>\n",
       "      <td>0.057018</td>\n",
       "      <td>...</td>\n",
       "      <td>0.156636</td>\n",
       "      <td>0.157495</td>\n",
       "      <td>0.157495</td>\n",
       "      <td>0.157495</td>\n",
       "      <td>0.029598</td>\n",
       "      <td>0.141111</td>\n",
       "      <td>0.165400</td>\n",
       "      <td>0.030007</td>\n",
       "      <td>0.141835</td>\n",
       "      <td>0.157539</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>0.418528</td>\n",
       "      <td>0.100406</td>\n",
       "      <td>0.011924</td>\n",
       "      <td>0.420103</td>\n",
       "      <td>0.107681</td>\n",
       "      <td>0.016044</td>\n",
       "      <td>0.038399</td>\n",
       "      <td>0.021178</td>\n",
       "      <td>0.002488</td>\n",
       "      <td>0.038069</td>\n",
       "      <td>...</td>\n",
       "      <td>0.035022</td>\n",
       "      <td>0.030968</td>\n",
       "      <td>0.030968</td>\n",
       "      <td>0.030968</td>\n",
       "      <td>0.002671</td>\n",
       "      <td>0.010831</td>\n",
       "      <td>0.008205</td>\n",
       "      <td>0.002452</td>\n",
       "      <td>0.009975</td>\n",
       "      <td>0.007621</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75788</th>\n",
       "      <td>0.410489</td>\n",
       "      <td>0.068855</td>\n",
       "      <td>0.002172</td>\n",
       "      <td>0.411563</td>\n",
       "      <td>0.075722</td>\n",
       "      <td>0.001446</td>\n",
       "      <td>0.034853</td>\n",
       "      <td>0.013227</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.035117</td>\n",
       "      <td>...</td>\n",
       "      <td>0.011539</td>\n",
       "      <td>0.011949</td>\n",
       "      <td>0.011949</td>\n",
       "      <td>0.011949</td>\n",
       "      <td>0.003616</td>\n",
       "      <td>0.012379</td>\n",
       "      <td>0.003407</td>\n",
       "      <td>0.002869</td>\n",
       "      <td>0.007624</td>\n",
       "      <td>0.003669</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75789</th>\n",
       "      <td>0.417518</td>\n",
       "      <td>0.097269</td>\n",
       "      <td>0.030530</td>\n",
       "      <td>0.418804</td>\n",
       "      <td>0.106710</td>\n",
       "      <td>0.038730</td>\n",
       "      <td>0.038545</td>\n",
       "      <td>0.033806</td>\n",
       "      <td>0.028812</td>\n",
       "      <td>0.038993</td>\n",
       "      <td>...</td>\n",
       "      <td>0.035293</td>\n",
       "      <td>0.038291</td>\n",
       "      <td>0.038291</td>\n",
       "      <td>0.038291</td>\n",
       "      <td>0.002790</td>\n",
       "      <td>0.032164</td>\n",
       "      <td>0.031453</td>\n",
       "      <td>0.001999</td>\n",
       "      <td>0.030000</td>\n",
       "      <td>0.028430</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75790</th>\n",
       "      <td>0.418702</td>\n",
       "      <td>0.101181</td>\n",
       "      <td>0.013066</td>\n",
       "      <td>0.420162</td>\n",
       "      <td>0.107688</td>\n",
       "      <td>0.015903</td>\n",
       "      <td>0.038694</td>\n",
       "      <td>0.022024</td>\n",
       "      <td>0.004388</td>\n",
       "      <td>0.038403</td>\n",
       "      <td>...</td>\n",
       "      <td>0.037074</td>\n",
       "      <td>0.033051</td>\n",
       "      <td>0.033051</td>\n",
       "      <td>0.033051</td>\n",
       "      <td>0.002671</td>\n",
       "      <td>0.010799</td>\n",
       "      <td>0.006575</td>\n",
       "      <td>0.002434</td>\n",
       "      <td>0.008950</td>\n",
       "      <td>0.006382</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75791</th>\n",
       "      <td>0.412599</td>\n",
       "      <td>0.079670</td>\n",
       "      <td>0.002407</td>\n",
       "      <td>0.413336</td>\n",
       "      <td>0.083996</td>\n",
       "      <td>0.010904</td>\n",
       "      <td>0.035617</td>\n",
       "      <td>0.017527</td>\n",
       "      <td>0.009378</td>\n",
       "      <td>0.035127</td>\n",
       "      <td>...</td>\n",
       "      <td>0.016180</td>\n",
       "      <td>0.012349</td>\n",
       "      <td>0.012349</td>\n",
       "      <td>0.012349</td>\n",
       "      <td>0.004025</td>\n",
       "      <td>0.018319</td>\n",
       "      <td>0.016058</td>\n",
       "      <td>0.004279</td>\n",
       "      <td>0.017183</td>\n",
       "      <td>0.013648</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75792</th>\n",
       "      <td>0.411548</td>\n",
       "      <td>0.076579</td>\n",
       "      <td>0.010528</td>\n",
       "      <td>0.411990</td>\n",
       "      <td>0.077877</td>\n",
       "      <td>0.009689</td>\n",
       "      <td>0.034534</td>\n",
       "      <td>0.010544</td>\n",
       "      <td>0.003598</td>\n",
       "      <td>0.034822</td>\n",
       "      <td>...</td>\n",
       "      <td>0.009462</td>\n",
       "      <td>0.010041</td>\n",
       "      <td>0.010041</td>\n",
       "      <td>0.010041</td>\n",
       "      <td>0.001489</td>\n",
       "      <td>0.005890</td>\n",
       "      <td>0.006204</td>\n",
       "      <td>0.001437</td>\n",
       "      <td>0.005399</td>\n",
       "      <td>0.006644</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75793</th>\n",
       "      <td>0.413389</td>\n",
       "      <td>0.085014</td>\n",
       "      <td>0.020779</td>\n",
       "      <td>0.413366</td>\n",
       "      <td>0.085813</td>\n",
       "      <td>0.022046</td>\n",
       "      <td>0.034918</td>\n",
       "      <td>0.018054</td>\n",
       "      <td>0.016537</td>\n",
       "      <td>0.035197</td>\n",
       "      <td>...</td>\n",
       "      <td>0.010364</td>\n",
       "      <td>0.011025</td>\n",
       "      <td>0.011025</td>\n",
       "      <td>0.011025</td>\n",
       "      <td>0.001493</td>\n",
       "      <td>0.014783</td>\n",
       "      <td>0.018544</td>\n",
       "      <td>0.001332</td>\n",
       "      <td>0.012742</td>\n",
       "      <td>0.015120</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75794</th>\n",
       "      <td>0.434982</td>\n",
       "      <td>0.194744</td>\n",
       "      <td>0.179907</td>\n",
       "      <td>0.433487</td>\n",
       "      <td>0.187355</td>\n",
       "      <td>0.176092</td>\n",
       "      <td>0.056312</td>\n",
       "      <td>0.150987</td>\n",
       "      <td>0.186166</td>\n",
       "      <td>0.054671</td>\n",
       "      <td>...</td>\n",
       "      <td>0.147397</td>\n",
       "      <td>0.143342</td>\n",
       "      <td>0.143342</td>\n",
       "      <td>0.143342</td>\n",
       "      <td>0.029655</td>\n",
       "      <td>0.144990</td>\n",
       "      <td>0.172896</td>\n",
       "      <td>0.030030</td>\n",
       "      <td>0.144198</td>\n",
       "      <td>0.172834</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75795</th>\n",
       "      <td>0.440943</td>\n",
       "      <td>0.244201</td>\n",
       "      <td>0.301835</td>\n",
       "      <td>0.436589</td>\n",
       "      <td>0.215384</td>\n",
       "      <td>0.257612</td>\n",
       "      <td>0.064343</td>\n",
       "      <td>0.219945</td>\n",
       "      <td>0.283409</td>\n",
       "      <td>0.061005</td>\n",
       "      <td>...</td>\n",
       "      <td>0.196651</td>\n",
       "      <td>0.183018</td>\n",
       "      <td>0.183018</td>\n",
       "      <td>0.183018</td>\n",
       "      <td>0.036364</td>\n",
       "      <td>0.192779</td>\n",
       "      <td>0.245876</td>\n",
       "      <td>0.039713</td>\n",
       "      <td>0.209551</td>\n",
       "      <td>0.274618</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75796</th>\n",
       "      <td>0.421691</td>\n",
       "      <td>0.114429</td>\n",
       "      <td>0.024911</td>\n",
       "      <td>0.423624</td>\n",
       "      <td>0.122955</td>\n",
       "      <td>0.029787</td>\n",
       "      <td>0.040785</td>\n",
       "      <td>0.040477</td>\n",
       "      <td>0.030473</td>\n",
       "      <td>0.040565</td>\n",
       "      <td>...</td>\n",
       "      <td>0.049073</td>\n",
       "      <td>0.045996</td>\n",
       "      <td>0.045996</td>\n",
       "      <td>0.045996</td>\n",
       "      <td>0.004912</td>\n",
       "      <td>0.019141</td>\n",
       "      <td>0.021122</td>\n",
       "      <td>0.004515</td>\n",
       "      <td>0.016701</td>\n",
       "      <td>0.018644</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75797</th>\n",
       "      <td>0.424963</td>\n",
       "      <td>0.136092</td>\n",
       "      <td>0.074365</td>\n",
       "      <td>0.423575</td>\n",
       "      <td>0.130961</td>\n",
       "      <td>0.086378</td>\n",
       "      <td>0.047185</td>\n",
       "      <td>0.081988</td>\n",
       "      <td>0.068337</td>\n",
       "      <td>0.047943</td>\n",
       "      <td>...</td>\n",
       "      <td>0.092608</td>\n",
       "      <td>0.095220</td>\n",
       "      <td>0.095220</td>\n",
       "      <td>0.095220</td>\n",
       "      <td>0.018728</td>\n",
       "      <td>0.107184</td>\n",
       "      <td>0.129391</td>\n",
       "      <td>0.016141</td>\n",
       "      <td>0.099052</td>\n",
       "      <td>0.117656</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75798</th>\n",
       "      <td>0.411549</td>\n",
       "      <td>0.074092</td>\n",
       "      <td>0.002306</td>\n",
       "      <td>0.412627</td>\n",
       "      <td>0.078842</td>\n",
       "      <td>0.006004</td>\n",
       "      <td>0.034581</td>\n",
       "      <td>0.011005</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.034587</td>\n",
       "      <td>...</td>\n",
       "      <td>0.009679</td>\n",
       "      <td>0.008307</td>\n",
       "      <td>0.008307</td>\n",
       "      <td>0.008307</td>\n",
       "      <td>0.000687</td>\n",
       "      <td>0.002913</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000354</td>\n",
       "      <td>0.001624</td>\n",
       "      <td>0.000650</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75799</th>\n",
       "      <td>0.411832</td>\n",
       "      <td>0.076808</td>\n",
       "      <td>0.003401</td>\n",
       "      <td>0.412155</td>\n",
       "      <td>0.078669</td>\n",
       "      <td>0.006655</td>\n",
       "      <td>0.034751</td>\n",
       "      <td>0.010661</td>\n",
       "      <td>0.000527</td>\n",
       "      <td>0.034861</td>\n",
       "      <td>...</td>\n",
       "      <td>0.010855</td>\n",
       "      <td>0.010459</td>\n",
       "      <td>0.010459</td>\n",
       "      <td>0.010459</td>\n",
       "      <td>0.000862</td>\n",
       "      <td>0.002583</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000915</td>\n",
       "      <td>0.002730</td>\n",
       "      <td>0.001857</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75800</th>\n",
       "      <td>0.413591</td>\n",
       "      <td>0.085599</td>\n",
       "      <td>0.028138</td>\n",
       "      <td>0.413916</td>\n",
       "      <td>0.086703</td>\n",
       "      <td>0.025942</td>\n",
       "      <td>0.037000</td>\n",
       "      <td>0.024176</td>\n",
       "      <td>0.024877</td>\n",
       "      <td>0.037327</td>\n",
       "      <td>...</td>\n",
       "      <td>0.025554</td>\n",
       "      <td>0.027936</td>\n",
       "      <td>0.027936</td>\n",
       "      <td>0.027936</td>\n",
       "      <td>0.001676</td>\n",
       "      <td>0.018044</td>\n",
       "      <td>0.018631</td>\n",
       "      <td>0.001624</td>\n",
       "      <td>0.018327</td>\n",
       "      <td>0.019189</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75801</th>\n",
       "      <td>0.413488</td>\n",
       "      <td>0.084705</td>\n",
       "      <td>0.023385</td>\n",
       "      <td>0.413833</td>\n",
       "      <td>0.086226</td>\n",
       "      <td>0.023582</td>\n",
       "      <td>0.037000</td>\n",
       "      <td>0.023162</td>\n",
       "      <td>0.022146</td>\n",
       "      <td>0.037314</td>\n",
       "      <td>...</td>\n",
       "      <td>0.025554</td>\n",
       "      <td>0.027936</td>\n",
       "      <td>0.027936</td>\n",
       "      <td>0.027936</td>\n",
       "      <td>0.001432</td>\n",
       "      <td>0.015849</td>\n",
       "      <td>0.016009</td>\n",
       "      <td>0.001306</td>\n",
       "      <td>0.015672</td>\n",
       "      <td>0.016363</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75802</th>\n",
       "      <td>0.427627</td>\n",
       "      <td>0.168440</td>\n",
       "      <td>0.184428</td>\n",
       "      <td>0.421577</td>\n",
       "      <td>0.137948</td>\n",
       "      <td>0.131063</td>\n",
       "      <td>0.043126</td>\n",
       "      <td>0.081362</td>\n",
       "      <td>0.171638</td>\n",
       "      <td>0.045379</td>\n",
       "      <td>...</td>\n",
       "      <td>0.057234</td>\n",
       "      <td>0.070418</td>\n",
       "      <td>0.070418</td>\n",
       "      <td>0.070418</td>\n",
       "      <td>0.003664</td>\n",
       "      <td>0.052909</td>\n",
       "      <td>0.105259</td>\n",
       "      <td>0.010726</td>\n",
       "      <td>0.071018</td>\n",
       "      <td>0.102450</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75803</th>\n",
       "      <td>0.422740</td>\n",
       "      <td>0.120773</td>\n",
       "      <td>0.042186</td>\n",
       "      <td>0.424318</td>\n",
       "      <td>0.129269</td>\n",
       "      <td>0.051493</td>\n",
       "      <td>0.042088</td>\n",
       "      <td>0.047499</td>\n",
       "      <td>0.046515</td>\n",
       "      <td>0.042530</td>\n",
       "      <td>...</td>\n",
       "      <td>0.057104</td>\n",
       "      <td>0.060223</td>\n",
       "      <td>0.060223</td>\n",
       "      <td>0.060223</td>\n",
       "      <td>0.004912</td>\n",
       "      <td>0.035438</td>\n",
       "      <td>0.043794</td>\n",
       "      <td>0.004490</td>\n",
       "      <td>0.034233</td>\n",
       "      <td>0.042670</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75804</th>\n",
       "      <td>0.422902</td>\n",
       "      <td>0.121421</td>\n",
       "      <td>0.028142</td>\n",
       "      <td>0.424513</td>\n",
       "      <td>0.129861</td>\n",
       "      <td>0.036070</td>\n",
       "      <td>0.042008</td>\n",
       "      <td>0.047451</td>\n",
       "      <td>0.034985</td>\n",
       "      <td>0.041790</td>\n",
       "      <td>...</td>\n",
       "      <td>0.057622</td>\n",
       "      <td>0.054296</td>\n",
       "      <td>0.054296</td>\n",
       "      <td>0.054296</td>\n",
       "      <td>0.006955</td>\n",
       "      <td>0.033228</td>\n",
       "      <td>0.040105</td>\n",
       "      <td>0.007350</td>\n",
       "      <td>0.031361</td>\n",
       "      <td>0.036115</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75805</th>\n",
       "      <td>0.414270</td>\n",
       "      <td>0.089059</td>\n",
       "      <td>0.027988</td>\n",
       "      <td>0.414412</td>\n",
       "      <td>0.090478</td>\n",
       "      <td>0.028631</td>\n",
       "      <td>0.037108</td>\n",
       "      <td>0.029445</td>\n",
       "      <td>0.028830</td>\n",
       "      <td>0.037456</td>\n",
       "      <td>...</td>\n",
       "      <td>0.025832</td>\n",
       "      <td>0.028283</td>\n",
       "      <td>0.028283</td>\n",
       "      <td>0.028283</td>\n",
       "      <td>0.008323</td>\n",
       "      <td>0.040994</td>\n",
       "      <td>0.043361</td>\n",
       "      <td>0.007182</td>\n",
       "      <td>0.037471</td>\n",
       "      <td>0.039321</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75806</th>\n",
       "      <td>0.422938</td>\n",
       "      <td>0.120876</td>\n",
       "      <td>0.050601</td>\n",
       "      <td>0.423343</td>\n",
       "      <td>0.133356</td>\n",
       "      <td>0.067874</td>\n",
       "      <td>0.043894</td>\n",
       "      <td>0.060188</td>\n",
       "      <td>0.080396</td>\n",
       "      <td>0.042959</td>\n",
       "      <td>...</td>\n",
       "      <td>0.069466</td>\n",
       "      <td>0.061145</td>\n",
       "      <td>0.061145</td>\n",
       "      <td>0.061145</td>\n",
       "      <td>0.015119</td>\n",
       "      <td>0.071911</td>\n",
       "      <td>0.072636</td>\n",
       "      <td>0.013428</td>\n",
       "      <td>0.077288</td>\n",
       "      <td>0.084816</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75807</th>\n",
       "      <td>0.410887</td>\n",
       "      <td>0.072750</td>\n",
       "      <td>0.004595</td>\n",
       "      <td>0.411463</td>\n",
       "      <td>0.074959</td>\n",
       "      <td>0.006193</td>\n",
       "      <td>0.034455</td>\n",
       "      <td>0.010444</td>\n",
       "      <td>0.002334</td>\n",
       "      <td>0.034666</td>\n",
       "      <td>...</td>\n",
       "      <td>0.008989</td>\n",
       "      <td>0.009260</td>\n",
       "      <td>0.009260</td>\n",
       "      <td>0.009260</td>\n",
       "      <td>0.000838</td>\n",
       "      <td>0.003410</td>\n",
       "      <td>0.001568</td>\n",
       "      <td>0.000704</td>\n",
       "      <td>0.002622</td>\n",
       "      <td>0.001448</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75808</th>\n",
       "      <td>0.417040</td>\n",
       "      <td>0.102681</td>\n",
       "      <td>0.070579</td>\n",
       "      <td>0.414655</td>\n",
       "      <td>0.095796</td>\n",
       "      <td>0.033871</td>\n",
       "      <td>0.036118</td>\n",
       "      <td>0.028593</td>\n",
       "      <td>0.044094</td>\n",
       "      <td>0.036724</td>\n",
       "      <td>...</td>\n",
       "      <td>0.019378</td>\n",
       "      <td>0.023192</td>\n",
       "      <td>0.023192</td>\n",
       "      <td>0.023192</td>\n",
       "      <td>0.004563</td>\n",
       "      <td>0.027866</td>\n",
       "      <td>0.044210</td>\n",
       "      <td>0.005099</td>\n",
       "      <td>0.028844</td>\n",
       "      <td>0.039854</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75809</th>\n",
       "      <td>0.434582</td>\n",
       "      <td>0.192667</td>\n",
       "      <td>0.169826</td>\n",
       "      <td>0.433105</td>\n",
       "      <td>0.185062</td>\n",
       "      <td>0.158949</td>\n",
       "      <td>0.054041</td>\n",
       "      <td>0.128651</td>\n",
       "      <td>0.150857</td>\n",
       "      <td>0.053948</td>\n",
       "      <td>...</td>\n",
       "      <td>0.134291</td>\n",
       "      <td>0.139006</td>\n",
       "      <td>0.139006</td>\n",
       "      <td>0.139006</td>\n",
       "      <td>0.029415</td>\n",
       "      <td>0.135643</td>\n",
       "      <td>0.147958</td>\n",
       "      <td>0.028768</td>\n",
       "      <td>0.137108</td>\n",
       "      <td>0.151805</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75810</th>\n",
       "      <td>0.413815</td>\n",
       "      <td>0.088864</td>\n",
       "      <td>0.003645</td>\n",
       "      <td>0.415258</td>\n",
       "      <td>0.089613</td>\n",
       "      <td>0.010979</td>\n",
       "      <td>0.035703</td>\n",
       "      <td>0.018006</td>\n",
       "      <td>0.002520</td>\n",
       "      <td>0.035849</td>\n",
       "      <td>...</td>\n",
       "      <td>0.016983</td>\n",
       "      <td>0.016283</td>\n",
       "      <td>0.016283</td>\n",
       "      <td>0.016283</td>\n",
       "      <td>0.003310</td>\n",
       "      <td>0.014068</td>\n",
       "      <td>0.009892</td>\n",
       "      <td>0.002975</td>\n",
       "      <td>0.011515</td>\n",
       "      <td>0.006604</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75811</th>\n",
       "      <td>0.413706</td>\n",
       "      <td>0.085604</td>\n",
       "      <td>0.023337</td>\n",
       "      <td>0.414048</td>\n",
       "      <td>0.087202</td>\n",
       "      <td>0.028502</td>\n",
       "      <td>0.036990</td>\n",
       "      <td>0.023022</td>\n",
       "      <td>0.021202</td>\n",
       "      <td>0.037295</td>\n",
       "      <td>...</td>\n",
       "      <td>0.025554</td>\n",
       "      <td>0.027936</td>\n",
       "      <td>0.027936</td>\n",
       "      <td>0.027936</td>\n",
       "      <td>0.001757</td>\n",
       "      <td>0.019500</td>\n",
       "      <td>0.020989</td>\n",
       "      <td>0.001808</td>\n",
       "      <td>0.018574</td>\n",
       "      <td>0.018640</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75812</th>\n",
       "      <td>0.410839</td>\n",
       "      <td>0.073293</td>\n",
       "      <td>0.001713</td>\n",
       "      <td>0.411364</td>\n",
       "      <td>0.074764</td>\n",
       "      <td>0.003901</td>\n",
       "      <td>0.035137</td>\n",
       "      <td>0.013169</td>\n",
       "      <td>0.006044</td>\n",
       "      <td>0.034920</td>\n",
       "      <td>...</td>\n",
       "      <td>0.013281</td>\n",
       "      <td>0.010993</td>\n",
       "      <td>0.010993</td>\n",
       "      <td>0.010993</td>\n",
       "      <td>0.003525</td>\n",
       "      <td>0.011643</td>\n",
       "      <td>0.009007</td>\n",
       "      <td>0.002365</td>\n",
       "      <td>0.007404</td>\n",
       "      <td>0.001748</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75813</th>\n",
       "      <td>0.428389</td>\n",
       "      <td>0.151673</td>\n",
       "      <td>0.063996</td>\n",
       "      <td>0.427447</td>\n",
       "      <td>0.146973</td>\n",
       "      <td>0.054993</td>\n",
       "      <td>0.046245</td>\n",
       "      <td>0.084145</td>\n",
       "      <td>0.086327</td>\n",
       "      <td>0.046556</td>\n",
       "      <td>...</td>\n",
       "      <td>0.082237</td>\n",
       "      <td>0.083134</td>\n",
       "      <td>0.083134</td>\n",
       "      <td>0.083134</td>\n",
       "      <td>0.007283</td>\n",
       "      <td>0.041277</td>\n",
       "      <td>0.065285</td>\n",
       "      <td>0.008544</td>\n",
       "      <td>0.051206</td>\n",
       "      <td>0.078243</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75814</th>\n",
       "      <td>0.412782</td>\n",
       "      <td>0.082549</td>\n",
       "      <td>0.043176</td>\n",
       "      <td>0.412969</td>\n",
       "      <td>0.083196</td>\n",
       "      <td>0.017974</td>\n",
       "      <td>0.034757</td>\n",
       "      <td>0.015849</td>\n",
       "      <td>0.010762</td>\n",
       "      <td>0.034997</td>\n",
       "      <td>...</td>\n",
       "      <td>0.009549</td>\n",
       "      <td>0.010194</td>\n",
       "      <td>0.010194</td>\n",
       "      <td>0.010194</td>\n",
       "      <td>0.001489</td>\n",
       "      <td>0.011138</td>\n",
       "      <td>0.012779</td>\n",
       "      <td>0.001444</td>\n",
       "      <td>0.011078</td>\n",
       "      <td>0.013744</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75815</th>\n",
       "      <td>0.410845</td>\n",
       "      <td>0.072529</td>\n",
       "      <td>0.004669</td>\n",
       "      <td>0.411450</td>\n",
       "      <td>0.074909</td>\n",
       "      <td>0.004777</td>\n",
       "      <td>0.034451</td>\n",
       "      <td>0.009409</td>\n",
       "      <td>0.000475</td>\n",
       "      <td>0.034654</td>\n",
       "      <td>...</td>\n",
       "      <td>0.008989</td>\n",
       "      <td>0.009204</td>\n",
       "      <td>0.009204</td>\n",
       "      <td>0.009204</td>\n",
       "      <td>0.000774</td>\n",
       "      <td>0.002292</td>\n",
       "      <td>0.000494</td>\n",
       "      <td>0.000539</td>\n",
       "      <td>0.001322</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75816</th>\n",
       "      <td>0.415539</td>\n",
       "      <td>0.095945</td>\n",
       "      <td>0.044377</td>\n",
       "      <td>0.414772</td>\n",
       "      <td>0.093151</td>\n",
       "      <td>0.041327</td>\n",
       "      <td>0.039590</td>\n",
       "      <td>0.044283</td>\n",
       "      <td>0.059742</td>\n",
       "      <td>0.039372</td>\n",
       "      <td>...</td>\n",
       "      <td>0.042060</td>\n",
       "      <td>0.041903</td>\n",
       "      <td>0.041903</td>\n",
       "      <td>0.041903</td>\n",
       "      <td>0.009830</td>\n",
       "      <td>0.054130</td>\n",
       "      <td>0.067994</td>\n",
       "      <td>0.010350</td>\n",
       "      <td>0.054921</td>\n",
       "      <td>0.068319</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75817</th>\n",
       "      <td>0.410677</td>\n",
       "      <td>0.071363</td>\n",
       "      <td>0.002133</td>\n",
       "      <td>0.411318</td>\n",
       "      <td>0.074146</td>\n",
       "      <td>0.003785</td>\n",
       "      <td>0.034451</td>\n",
       "      <td>0.008852</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.034411</td>\n",
       "      <td>...</td>\n",
       "      <td>0.008989</td>\n",
       "      <td>0.007423</td>\n",
       "      <td>0.007423</td>\n",
       "      <td>0.007423</td>\n",
       "      <td>0.000779</td>\n",
       "      <td>0.002119</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000652</td>\n",
       "      <td>0.001622</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>75818 rows × 24 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       xgboost_p1  xgboost_p2  xgboost_p3  xgboost_p4  xgboost_p5  xgboost_p6  \\\n",
       "0        0.415721    0.097202    0.042828    0.415151    0.095824    0.051976   \n",
       "1        0.416529    0.100956    0.052814    0.415314    0.097803    0.055960   \n",
       "2        0.411698    0.075681    0.002523    0.412123    0.078379    0.005697   \n",
       "3        0.418683    0.101366    0.014062    0.420119    0.107787    0.017989   \n",
       "4        0.411532    0.075520    0.003606    0.412088    0.078156    0.006529   \n",
       "5        0.436388    0.207404    0.223897    0.434440    0.197179    0.219029   \n",
       "6        0.417094    0.102855    0.038031    0.417951    0.108334    0.022224   \n",
       "7        0.434541    0.188525    0.165028    0.433910    0.188646    0.164309   \n",
       "8        0.412955    0.084585    0.023091    0.413026    0.084503    0.027572   \n",
       "9        0.413165    0.083132    0.012546    0.413612    0.086278    0.021072   \n",
       "10       0.413652    0.085491    0.026448    0.414012    0.086991    0.026322   \n",
       "11       0.412398    0.079559    0.005860    0.412624    0.081404    0.009184   \n",
       "12       0.413001    0.083037    0.009617    0.413477    0.084387    0.018354   \n",
       "13       0.411574    0.076538    0.010394    0.412024    0.078091    0.010398   \n",
       "14       0.411586    0.075089    0.034677    0.412344    0.078928    0.004842   \n",
       "15       0.414358    0.087206    0.055722    0.414715    0.090410    0.022342   \n",
       "16       0.425593    0.152285    0.140102    0.423683    0.141453    0.138531   \n",
       "17       0.411404    0.075149    0.004505    0.411951    0.077534    0.006633   \n",
       "18       0.411590    0.076757    0.012043    0.412022    0.078281    0.012488   \n",
       "19       0.412702    0.082277    0.009676    0.413057    0.084306    0.017678   \n",
       "20       0.413684    0.085181    0.021388    0.414113    0.087387    0.025052   \n",
       "21       0.434643    0.193682    0.180755    0.433105    0.185280    0.172130   \n",
       "22       0.423064    0.123198    0.031013    0.424802    0.130981    0.036904   \n",
       "23       0.411123    0.072142    0.001795    0.412533    0.078629    0.002506   \n",
       "24       0.420855    0.119755    0.034098    0.423301    0.128813    0.070650   \n",
       "25       0.412449    0.078390    0.002095    0.413154    0.082205    0.009346   \n",
       "26       0.411448    0.075552    0.004883    0.411983    0.077790    0.007308   \n",
       "27       0.411415    0.075387    0.008137    0.411955    0.077557    0.007026   \n",
       "28       0.436208    0.201367    0.174822    0.434546    0.193467    0.175310   \n",
       "29       0.418528    0.100406    0.011924    0.420103    0.107681    0.016044   \n",
       "...           ...         ...         ...         ...         ...         ...   \n",
       "75788    0.410489    0.068855    0.002172    0.411563    0.075722    0.001446   \n",
       "75789    0.417518    0.097269    0.030530    0.418804    0.106710    0.038730   \n",
       "75790    0.418702    0.101181    0.013066    0.420162    0.107688    0.015903   \n",
       "75791    0.412599    0.079670    0.002407    0.413336    0.083996    0.010904   \n",
       "75792    0.411548    0.076579    0.010528    0.411990    0.077877    0.009689   \n",
       "75793    0.413389    0.085014    0.020779    0.413366    0.085813    0.022046   \n",
       "75794    0.434982    0.194744    0.179907    0.433487    0.187355    0.176092   \n",
       "75795    0.440943    0.244201    0.301835    0.436589    0.215384    0.257612   \n",
       "75796    0.421691    0.114429    0.024911    0.423624    0.122955    0.029787   \n",
       "75797    0.424963    0.136092    0.074365    0.423575    0.130961    0.086378   \n",
       "75798    0.411549    0.074092    0.002306    0.412627    0.078842    0.006004   \n",
       "75799    0.411832    0.076808    0.003401    0.412155    0.078669    0.006655   \n",
       "75800    0.413591    0.085599    0.028138    0.413916    0.086703    0.025942   \n",
       "75801    0.413488    0.084705    0.023385    0.413833    0.086226    0.023582   \n",
       "75802    0.427627    0.168440    0.184428    0.421577    0.137948    0.131063   \n",
       "75803    0.422740    0.120773    0.042186    0.424318    0.129269    0.051493   \n",
       "75804    0.422902    0.121421    0.028142    0.424513    0.129861    0.036070   \n",
       "75805    0.414270    0.089059    0.027988    0.414412    0.090478    0.028631   \n",
       "75806    0.422938    0.120876    0.050601    0.423343    0.133356    0.067874   \n",
       "75807    0.410887    0.072750    0.004595    0.411463    0.074959    0.006193   \n",
       "75808    0.417040    0.102681    0.070579    0.414655    0.095796    0.033871   \n",
       "75809    0.434582    0.192667    0.169826    0.433105    0.185062    0.158949   \n",
       "75810    0.413815    0.088864    0.003645    0.415258    0.089613    0.010979   \n",
       "75811    0.413706    0.085604    0.023337    0.414048    0.087202    0.028502   \n",
       "75812    0.410839    0.073293    0.001713    0.411364    0.074764    0.003901   \n",
       "75813    0.428389    0.151673    0.063996    0.427447    0.146973    0.054993   \n",
       "75814    0.412782    0.082549    0.043176    0.412969    0.083196    0.017974   \n",
       "75815    0.410845    0.072529    0.004669    0.411450    0.074909    0.004777   \n",
       "75816    0.415539    0.095945    0.044377    0.414772    0.093151    0.041327   \n",
       "75817    0.410677    0.071363    0.002133    0.411318    0.074146    0.003785   \n",
       "\n",
       "       lightgbm_p1  lightgbm_p2  lightgbm_p3  lightgbm_p4     ...       \\\n",
       "0         0.038326     0.036761     0.044406     0.038865     ...        \n",
       "1         0.039659     0.042465     0.063659     0.040003     ...        \n",
       "2         0.034558     0.009907     0.000417     0.034524     ...        \n",
       "3         0.038399     0.022037     0.006973     0.038076     ...        \n",
       "4         0.034534     0.010064     0.001004     0.034572     ...        \n",
       "5         0.059274     0.171075     0.225850     0.057470     ...        \n",
       "6         0.043018     0.063572     0.056381     0.043660     ...        \n",
       "7         0.053870     0.128298     0.159471     0.055220     ...        \n",
       "8         0.035828     0.022094     0.021774     0.036197     ...        \n",
       "9         0.035916     0.021287     0.022722     0.036292     ...        \n",
       "10        0.037136     0.025133     0.026086     0.037477     ...        \n",
       "11        0.034700     0.012075     0.004345     0.034746     ...        \n",
       "12        0.035928     0.022697     0.022236     0.036316     ...        \n",
       "13        0.034538     0.012004     0.007478     0.034834     ...        \n",
       "14        0.035008     0.017095     0.018469     0.035232     ...        \n",
       "15        0.036133     0.024514     0.024004     0.035950     ...        \n",
       "16        0.047131     0.098496     0.126048     0.046789     ...        \n",
       "17        0.034534     0.010024     0.001414     0.034572     ...        \n",
       "18        0.034538     0.012548     0.011136     0.034834     ...        \n",
       "19        0.035805     0.019002     0.018259     0.036168     ...        \n",
       "20        0.036990     0.022865     0.019262     0.037292     ...        \n",
       "21        0.055687     0.148467     0.191709     0.054306     ...        \n",
       "22        0.041722     0.048125     0.033073     0.041093     ...        \n",
       "23        0.035297     0.018038     0.009790     0.035162     ...        \n",
       "24        0.043696     0.069506     0.087991     0.046068     ...        \n",
       "25        0.035651     0.017027     0.007619     0.035306     ...        \n",
       "26        0.034534     0.010024     0.001606     0.034572     ...        \n",
       "27        0.034534     0.010024     0.001483     0.034572     ...        \n",
       "28        0.057762     0.155047     0.178947     0.057018     ...        \n",
       "29        0.038399     0.021178     0.002488     0.038069     ...        \n",
       "...            ...          ...          ...          ...     ...        \n",
       "75788     0.034853     0.013227     0.000000     0.035117     ...        \n",
       "75789     0.038545     0.033806     0.028812     0.038993     ...        \n",
       "75790     0.038694     0.022024     0.004388     0.038403     ...        \n",
       "75791     0.035617     0.017527     0.009378     0.035127     ...        \n",
       "75792     0.034534     0.010544     0.003598     0.034822     ...        \n",
       "75793     0.034918     0.018054     0.016537     0.035197     ...        \n",
       "75794     0.056312     0.150987     0.186166     0.054671     ...        \n",
       "75795     0.064343     0.219945     0.283409     0.061005     ...        \n",
       "75796     0.040785     0.040477     0.030473     0.040565     ...        \n",
       "75797     0.047185     0.081988     0.068337     0.047943     ...        \n",
       "75798     0.034581     0.011005     0.000000     0.034587     ...        \n",
       "75799     0.034751     0.010661     0.000527     0.034861     ...        \n",
       "75800     0.037000     0.024176     0.024877     0.037327     ...        \n",
       "75801     0.037000     0.023162     0.022146     0.037314     ...        \n",
       "75802     0.043126     0.081362     0.171638     0.045379     ...        \n",
       "75803     0.042088     0.047499     0.046515     0.042530     ...        \n",
       "75804     0.042008     0.047451     0.034985     0.041790     ...        \n",
       "75805     0.037108     0.029445     0.028830     0.037456     ...        \n",
       "75806     0.043894     0.060188     0.080396     0.042959     ...        \n",
       "75807     0.034455     0.010444     0.002334     0.034666     ...        \n",
       "75808     0.036118     0.028593     0.044094     0.036724     ...        \n",
       "75809     0.054041     0.128651     0.150857     0.053948     ...        \n",
       "75810     0.035703     0.018006     0.002520     0.035849     ...        \n",
       "75811     0.036990     0.023022     0.021202     0.037295     ...        \n",
       "75812     0.035137     0.013169     0.006044     0.034920     ...        \n",
       "75813     0.046245     0.084145     0.086327     0.046556     ...        \n",
       "75814     0.034757     0.015849     0.010762     0.034997     ...        \n",
       "75815     0.034451     0.009409     0.000475     0.034654     ...        \n",
       "75816     0.039590     0.044283     0.059742     0.039372     ...        \n",
       "75817     0.034451     0.008852     0.000000     0.034411     ...        \n",
       "\n",
       "       lightgbm_p9  lightgbm_p10  lightgbm_p11  lightgbm_p12  catboost_p1  \\\n",
       "0         0.033345      0.037256      0.037256      0.037256     0.009733   \n",
       "1         0.043101      0.045748      0.045748      0.045748     0.009770   \n",
       "2         0.009579      0.008015      0.008015      0.008015     0.001534   \n",
       "3         0.035022      0.030968      0.030968      0.030968     0.002671   \n",
       "4         0.009462      0.008250      0.008250      0.008250     0.001609   \n",
       "5         0.166743      0.162629      0.162629      0.162629     0.036364   \n",
       "6         0.060888      0.065457      0.065457      0.065457     0.008193   \n",
       "7         0.131200      0.147904      0.147904      0.147904     0.035928   \n",
       "8         0.017801      0.019401      0.019401      0.019401     0.004148   \n",
       "9         0.018326      0.020287      0.020287      0.020287     0.004147   \n",
       "10        0.026272      0.028518      0.028518      0.028518     0.001475   \n",
       "11        0.010277      0.009081      0.009081      0.009081     0.001777   \n",
       "12        0.018324      0.020237      0.020237      0.020237     0.004193   \n",
       "13        0.009462      0.010097      0.010097      0.010097     0.001620   \n",
       "14        0.012412      0.012376      0.012376      0.012376     0.004563   \n",
       "15        0.019450      0.017882      0.017882      0.017882     0.003745   \n",
       "16        0.088449      0.089007      0.089007      0.089007     0.023202   \n",
       "17        0.009462      0.008250      0.008250      0.008250     0.001432   \n",
       "18        0.009462      0.010097      0.010097      0.010097     0.001737   \n",
       "19        0.017733      0.019200      0.019200      0.019200     0.003252   \n",
       "20        0.025554      0.027936      0.027936      0.027936     0.001676   \n",
       "21        0.143836      0.140858      0.140858      0.140858     0.029655   \n",
       "22        0.054546      0.050185      0.050185      0.050185     0.006553   \n",
       "23        0.014269      0.012509      0.012509      0.012509     0.003580   \n",
       "24        0.067733      0.085264      0.085264      0.085264     0.019712   \n",
       "25        0.016286      0.013085      0.013085      0.013085     0.003020   \n",
       "26        0.009462      0.008250      0.008250      0.008250     0.001472   \n",
       "27        0.009462      0.008250      0.008250      0.008250     0.001438   \n",
       "28        0.156636      0.157495      0.157495      0.157495     0.029598   \n",
       "29        0.035022      0.030968      0.030968      0.030968     0.002671   \n",
       "...            ...           ...           ...           ...          ...   \n",
       "75788     0.011539      0.011949      0.011949      0.011949     0.003616   \n",
       "75789     0.035293      0.038291      0.038291      0.038291     0.002790   \n",
       "75790     0.037074      0.033051      0.033051      0.033051     0.002671   \n",
       "75791     0.016180      0.012349      0.012349      0.012349     0.004025   \n",
       "75792     0.009462      0.010041      0.010041      0.010041     0.001489   \n",
       "75793     0.010364      0.011025      0.011025      0.011025     0.001493   \n",
       "75794     0.147397      0.143342      0.143342      0.143342     0.029655   \n",
       "75795     0.196651      0.183018      0.183018      0.183018     0.036364   \n",
       "75796     0.049073      0.045996      0.045996      0.045996     0.004912   \n",
       "75797     0.092608      0.095220      0.095220      0.095220     0.018728   \n",
       "75798     0.009679      0.008307      0.008307      0.008307     0.000687   \n",
       "75799     0.010855      0.010459      0.010459      0.010459     0.000862   \n",
       "75800     0.025554      0.027936      0.027936      0.027936     0.001676   \n",
       "75801     0.025554      0.027936      0.027936      0.027936     0.001432   \n",
       "75802     0.057234      0.070418      0.070418      0.070418     0.003664   \n",
       "75803     0.057104      0.060223      0.060223      0.060223     0.004912   \n",
       "75804     0.057622      0.054296      0.054296      0.054296     0.006955   \n",
       "75805     0.025832      0.028283      0.028283      0.028283     0.008323   \n",
       "75806     0.069466      0.061145      0.061145      0.061145     0.015119   \n",
       "75807     0.008989      0.009260      0.009260      0.009260     0.000838   \n",
       "75808     0.019378      0.023192      0.023192      0.023192     0.004563   \n",
       "75809     0.134291      0.139006      0.139006      0.139006     0.029415   \n",
       "75810     0.016983      0.016283      0.016283      0.016283     0.003310   \n",
       "75811     0.025554      0.027936      0.027936      0.027936     0.001757   \n",
       "75812     0.013281      0.010993      0.010993      0.010993     0.003525   \n",
       "75813     0.082237      0.083134      0.083134      0.083134     0.007283   \n",
       "75814     0.009549      0.010194      0.010194      0.010194     0.001489   \n",
       "75815     0.008989      0.009204      0.009204      0.009204     0.000774   \n",
       "75816     0.042060      0.041903      0.041903      0.041903     0.009830   \n",
       "75817     0.008989      0.007423      0.007423      0.007423     0.000779   \n",
       "\n",
       "       catboost_p2  catboost_p3  catboost_p4  catboost_p5  catboost_p6  \n",
       "0         0.047941     0.056483     0.008408     0.044714     0.050532  \n",
       "1         0.048732     0.057247     0.008675     0.047394     0.054229  \n",
       "2         0.005833     0.004568     0.001419     0.004775     0.002751  \n",
       "3         0.010831     0.010048     0.002452     0.009966     0.009669  \n",
       "4         0.005400     0.003600     0.001456     0.004505     0.002639  \n",
       "5         0.191683     0.229837     0.039539     0.197686     0.235108  \n",
       "6         0.047765     0.065961     0.011391     0.060371     0.066223  \n",
       "7         0.138741     0.158281     0.037659     0.140164     0.157870  \n",
       "8         0.025217     0.029418     0.004187     0.022857     0.029823  \n",
       "9         0.024443     0.027147     0.004252     0.022383     0.022125  \n",
       "10        0.018881     0.021307     0.001319     0.017480     0.018862  \n",
       "11        0.006913     0.006437     0.001745     0.005949     0.005329  \n",
       "12        0.020361     0.019274     0.004568     0.018458     0.019071  \n",
       "13        0.007365     0.008802     0.001521     0.006905     0.008265  \n",
       "14        0.020259     0.023014     0.004628     0.018797     0.022652  \n",
       "15        0.013890     0.022812     0.004255     0.016833     0.026008  \n",
       "16        0.110588     0.134165     0.019099     0.104189     0.132489  \n",
       "17        0.005308     0.003673     0.001302     0.004738     0.003311  \n",
       "18        0.008664     0.011906     0.001757     0.008469     0.012168  \n",
       "19        0.016323     0.021822     0.002777     0.016189     0.021345  \n",
       "20        0.017952     0.017277     0.001662     0.017084     0.016626  \n",
       "21        0.145261     0.172555     0.030307     0.147457     0.176705  \n",
       "22        0.028691     0.031953     0.006743     0.029492     0.035461  \n",
       "23        0.014593     0.015703     0.003407     0.013434     0.012804  \n",
       "24        0.095134     0.107975     0.016086     0.080713     0.095997  \n",
       "25        0.011742     0.008326     0.002444     0.009733     0.002689  \n",
       "26        0.005631     0.004521     0.001389     0.005615     0.004949  \n",
       "27        0.005665     0.004682     0.001303     0.005046     0.004602  \n",
       "28        0.141111     0.165400     0.030007     0.141835     0.157539  \n",
       "29        0.010831     0.008205     0.002452     0.009975     0.007621  \n",
       "...            ...          ...          ...          ...          ...  \n",
       "75788     0.012379     0.003407     0.002869     0.007624     0.003669  \n",
       "75789     0.032164     0.031453     0.001999     0.030000     0.028430  \n",
       "75790     0.010799     0.006575     0.002434     0.008950     0.006382  \n",
       "75791     0.018319     0.016058     0.004279     0.017183     0.013648  \n",
       "75792     0.005890     0.006204     0.001437     0.005399     0.006644  \n",
       "75793     0.014783     0.018544     0.001332     0.012742     0.015120  \n",
       "75794     0.144990     0.172896     0.030030     0.144198     0.172834  \n",
       "75795     0.192779     0.245876     0.039713     0.209551     0.274618  \n",
       "75796     0.019141     0.021122     0.004515     0.016701     0.018644  \n",
       "75797     0.107184     0.129391     0.016141     0.099052     0.117656  \n",
       "75798     0.002913     0.000000     0.000354     0.001624     0.000650  \n",
       "75799     0.002583     0.000000     0.000915     0.002730     0.001857  \n",
       "75800     0.018044     0.018631     0.001624     0.018327     0.019189  \n",
       "75801     0.015849     0.016009     0.001306     0.015672     0.016363  \n",
       "75802     0.052909     0.105259     0.010726     0.071018     0.102450  \n",
       "75803     0.035438     0.043794     0.004490     0.034233     0.042670  \n",
       "75804     0.033228     0.040105     0.007350     0.031361     0.036115  \n",
       "75805     0.040994     0.043361     0.007182     0.037471     0.039321  \n",
       "75806     0.071911     0.072636     0.013428     0.077288     0.084816  \n",
       "75807     0.003410     0.001568     0.000704     0.002622     0.001448  \n",
       "75808     0.027866     0.044210     0.005099     0.028844     0.039854  \n",
       "75809     0.135643     0.147958     0.028768     0.137108     0.151805  \n",
       "75810     0.014068     0.009892     0.002975     0.011515     0.006604  \n",
       "75811     0.019500     0.020989     0.001808     0.018574     0.018640  \n",
       "75812     0.011643     0.009007     0.002365     0.007404     0.001748  \n",
       "75813     0.041277     0.065285     0.008544     0.051206     0.078243  \n",
       "75814     0.011138     0.012779     0.001444     0.011078     0.013744  \n",
       "75815     0.002292     0.000494     0.000539     0.001322     0.000000  \n",
       "75816     0.054130     0.067994     0.010350     0.054921     0.068319  \n",
       "75817     0.002119     0.000000     0.000652     0.001622     0.000000  \n",
       "\n",
       "[75818 rows x 24 columns]"
      ]
     },
     "execution_count": 147,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Juntando as Meta Features do XGBoost - LightGBM e CatBoost\n",
    "meta_features_train = pd.concat([preds_xgboost_train, preds_lgbm_train, preds_catboost_train], axis=1)\n",
    "meta_features_test = pd.concat([preds_xgboost_test, preds_lgbm_test, preds_catboost_test], axis=1)\n",
    "\n",
    "# Salvando as Meta Features em um arquivo .csv\n",
    "meta_features_train.to_csv('/users/diegobernardo/downloads/base-santander/meta_features_train_001.csv', sep=',', index=False)\n",
    "meta_features_test.to_csv('/users/diegobernardo/downloads/base-santander/meta_features_test_001.csv', sep=',', index=False)\n",
    "\n",
    "meta_features_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "####################################################################################################################\n",
    "########################################     FIM DO PRIMEIRO NÍVEL     #############################################\n",
    "####################################################################################################################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {},
   "outputs": [],
   "source": [
    "####################################################################################################################\n",
    "########################################     INÍCIO DO SEGUNDO NÍVEL     #############################################\n",
    "####################################################################################################################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Segundo nível\n",
    "# Regressão linear utilizando somente as Meta Features\n",
    "\n",
    "model = linear_model.LinearRegression().fit(meta_features_train, train_target)\n",
    "output = model.predict(meta_features_test)\n",
    "output[output < 0] = 0\n",
    "output[output > 1] = 1\n",
    "\n",
    "df_output = pd.DataFrame({'ID':test_id, 'TARGET':output})\n",
    "df_output.to_csv('/users/diegobernardo/downloads/base-santander/output_stacking_001.csv', sep=',', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'meta_features_train' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-99-6adb8d587338>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;31m# Regressão linear utilizando somente as Meta Features\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlinear_model\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mLinearRegression\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmeta_features_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_target\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m \u001b[0mpred_linear\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmeta_features_test\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0mpred_linear\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mpred_linear\u001b[0m \u001b[0;34m<\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'meta_features_train' is not defined"
     ]
    }
   ],
   "source": [
    "# Segundo nível\n",
    "# Regressão linear utilizando somente as Meta Features \n",
    "\n",
    "model = linear_model.LinearRegression().fit(meta_features_train, train_target)\n",
    "pred_linear = model.predict(meta_features_test)\n",
    "pred_linear[pred_linear < 0] = 0\n",
    "pred_linear[pred_linear > 1] = 1\n",
    "\n",
    "\n",
    "# XGBoost utilizando Meta Features e Features originais\n",
    "param = { 'objective':'reg:logistic', 'booster':'gbtree', 'eta':0.01, 'max_depth':3, 'subsample':0.9, 'silent':True, 'colsample_bytree':0.50, 'colsample_bylevel':0.20, 'eval_metric':'auc', 'seed': 1990}\n",
    "data_train =  pd.concat([df_train, meta_features_train], axis=1)\n",
    "data_test  =  pd.concat([df_test,  meta_features_test], axis=1)\n",
    "data_train_xgboost = xgb.DMatrix(data=data_train, label=train_target, weight=data_train['var15'])\n",
    "data_test_xgboost = xgb.DMatrix(data=data_test, weight=data_test['var15'])\n",
    "\n",
    "preds_param = np.zeros(data_test_xgboost.num_row())\n",
    "    \n",
    "# Loop para retirar o ruído\n",
    "for i in range(n_iteracoes):\n",
    "    param['seed'] = 1990+i\n",
    "    model_xgboost = xgb.train(param, data_train_xgboost, num_round)\n",
    "    preds_param += model_xgboost.predict(data_test_xgboost)\n",
    "    \n",
    "pred_xgboost /= n_iteracoes\n",
    "pred_xgboost[pred_xgboost < 0] = 0\n",
    "pred_xgboost[pred_xgboost > 1] = 1\n",
    "\n",
    "# Juntando Regressão linear e XGBoost\n",
    "# Regressão linear com peso 2 e XGBoost com peso 1\n",
    "output = ((pred_linear*2) + pred_xgboost) /3\n",
    "output[output < 0] = 0\n",
    "output[output > 1] = 1\n",
    "\n",
    "df_output = pd.DataFrame({'ID':test_id, 'TARGET':output})\n",
    "df_output.to_csv('/users/diegobernardo/downloads/base-santander/output_stacking_001.csv', sep=',', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "369"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "teste1 = np.array([3,6,9,12])\n",
    "teste2 = np.array([1,2,3,4])\n",
    "teste = ((teste1*2)+teste2)/3\n",
    "#teste\n",
    "x_train = np.array(df_train)\n",
    "x_train.shape[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/30\n",
      "76020/76020 [==============================] - 4s 59us/step - loss: 0.0434 - acc: 0.9555\n",
      "Epoch 2/30\n",
      "76020/76020 [==============================] - 2s 30us/step - loss: 0.0406 - acc: 0.9590\n",
      "Epoch 3/30\n",
      "76020/76020 [==============================] - 2s 30us/step - loss: 0.0399 - acc: 0.9596\n",
      "Epoch 4/30\n",
      "76020/76020 [==============================] - 2s 30us/step - loss: 0.0394 - acc: 0.9598\n",
      "Epoch 5/30\n",
      "76020/76020 [==============================] - 2s 30us/step - loss: 0.0393 - acc: 0.9600\n",
      "Epoch 6/30\n",
      "76020/76020 [==============================] - 2s 29us/step - loss: 0.0390 - acc: 0.9601\n",
      "Epoch 7/30\n",
      "76020/76020 [==============================] - 2s 29us/step - loss: 0.0391 - acc: 0.9600\n",
      "Epoch 8/30\n",
      "76020/76020 [==============================] - 2s 30us/step - loss: 0.0389 - acc: 0.9602\n",
      "Epoch 9/30\n",
      "76020/76020 [==============================] - 2s 30us/step - loss: 0.0388 - acc: 0.9602\n",
      "Epoch 10/30\n",
      "76020/76020 [==============================] - 2s 30us/step - loss: 0.0388 - acc: 0.9602\n",
      "Epoch 11/30\n",
      "76020/76020 [==============================] - 2s 29us/step - loss: 0.0387 - acc: 0.9603\n",
      "Epoch 12/30\n",
      "76020/76020 [==============================] - 2s 30us/step - loss: 0.0387 - acc: 0.9603\n",
      "Epoch 13/30\n",
      "76020/76020 [==============================] - 2s 30us/step - loss: 0.0387 - acc: 0.9603\n",
      "Epoch 14/30\n",
      "76020/76020 [==============================] - 2s 30us/step - loss: 0.0387 - acc: 0.9603\n",
      "Epoch 15/30\n",
      "76020/76020 [==============================] - 2s 30us/step - loss: 0.0387 - acc: 0.9603\n",
      "Epoch 16/30\n",
      "76020/76020 [==============================] - 2s 30us/step - loss: 0.0387 - acc: 0.9603\n",
      "Epoch 17/30\n",
      "76020/76020 [==============================] - 2s 30us/step - loss: 0.0387 - acc: 0.9603\n",
      "Epoch 18/30\n",
      "76020/76020 [==============================] - 2s 30us/step - loss: 0.0386 - acc: 0.9604\n",
      "Epoch 19/30\n",
      "76020/76020 [==============================] - 2s 30us/step - loss: 0.0386 - acc: 0.9604\n",
      "Epoch 20/30\n",
      "76020/76020 [==============================] - 3s 35us/step - loss: 0.0386 - acc: 0.9603\n",
      "Epoch 21/30\n",
      "76020/76020 [==============================] - 2s 31us/step - loss: 0.0386 - acc: 0.9604\n",
      "Epoch 22/30\n",
      "76020/76020 [==============================] - 2s 30us/step - loss: 0.0386 - acc: 0.9603\n",
      "Epoch 23/30\n",
      "76020/76020 [==============================] - 3s 33us/step - loss: 0.0386 - acc: 0.9604\n",
      "Epoch 24/30\n",
      "76020/76020 [==============================] - 4s 51us/step - loss: 0.0386 - acc: 0.9604\n",
      "Epoch 25/30\n",
      "76020/76020 [==============================] - 4s 50us/step - loss: 0.0386 - acc: 0.9604\n",
      "Epoch 26/30\n",
      "76020/76020 [==============================] - 3s 36us/step - loss: 0.0386 - acc: 0.9604\n",
      "Epoch 27/30\n",
      "76020/76020 [==============================] - 3s 34us/step - loss: 0.0386 - acc: 0.9604\n",
      "Epoch 28/30\n",
      "76020/76020 [==============================] - 3s 34us/step - loss: 0.0385 - acc: 0.9604\n",
      "Epoch 29/30\n",
      "76020/76020 [==============================] - 3s 35us/step - loss: 0.0385 - acc: 0.9604\n",
      "Epoch 30/30\n",
      "76020/76020 [==============================] - 3s 34us/step - loss: 0.0385 - acc: 0.9604\n",
      "75818/75818 [==============================] - 1s 14us/step\n"
     ]
    }
   ],
   "source": [
    "#############\n",
    "### Keras ###\n",
    "#############\n",
    "\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Activation, Dropout\n",
    "\n",
    "# Convertendo os dados para um formato que o Keras trabalha\n",
    "x_teste = np.array(df_test)\n",
    "x_train = np.array(df_train)\n",
    "y_train = np.array(train_target)\n",
    "\n",
    "#Crie um modelo sequencial\n",
    "model_nn = Sequential()\n",
    "\n",
    "# 1a Camada - Adicione uma camada de entrada de 32 nós com o mesmo formato de entrada que as amostras de treinamento\n",
    "model_nn.add(Dense(32, input_dim=x_train.shape[1]))\n",
    "\n",
    "# 2a Camada - Adicione uma camada com 128 nós e ativação Tanh\n",
    "model_nn.add(Dense(128, activation='tanh'))\n",
    "model_nn.add(Dropout(0.2))\n",
    "\n",
    "# 3a Camada - Adicione uma camada com 128 nós e ativação Tanh\n",
    "model_nn.add(Dense(64, activation='tanh'))\n",
    "model_nn.add(Dropout(0.2))\n",
    "\n",
    "# 4a Camada - Adicione uma camada de saída completamente conectada\n",
    "model_nn.add(Dense(1))\n",
    "\n",
    "# 5a Camada - Adicione uma camada de ativação sigmóide\n",
    "model_nn.add(Activation('sigmoid'))\n",
    "\n",
    "# Compilando o modelo\n",
    "model_nn.compile(loss=\"mean_squared_error\", optimizer=\"sgd\", metrics = [\"accuracy\"])\n",
    "\n",
    "# Treinando o modelo\n",
    "model_nn.fit(x_train, y_train, epochs=50, verbose=0, batch_size=64)\n",
    "\n",
    "# Testando o modelo\n",
    "pred_neural_network = model_nn.predict(x_teste, batch_size=64, verbose=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "       1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "       1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "       1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "       1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "       1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "       1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "       1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "       1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "       1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "       1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "       1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "       1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "       1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "       1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "       1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "       1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "       1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "       1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "       1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "       1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "       1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "       1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "       1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "       1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "       1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "       1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "       1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "       1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "       1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.],\n",
       "      dtype=float32)"
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "teste = pred_neural_network[pred_neural_network > 0]\n",
    "len(teste)\n",
    "teste"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
